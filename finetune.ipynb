{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c57b02",
   "metadata": {},
   "source": [
    "# Complete Code for Fine-tuning GPT-4.1-mini on Shadertoy Top1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910c954",
   "metadata": {},
   "source": [
    "## 1. Setup and Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7dc4787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 24.11.3\n",
      "    latest version: 25.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/soham/miniforge3/envs/iterative-shader-lab\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.12\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    expat-2.7.1                |       h313beb8_0         156 KB\n",
      "    python-3.12.9              |       h99e199e_0        13.9 MB\n",
      "    setuptools-78.1.1          |  py312hca03da5_0         2.2 MB\n",
      "    wheel-0.45.1               |  py312hca03da5_0         148 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        16.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  bzip2              pkgs/main/osx-arm64::bzip2-1.0.8-h80987f9_6 \n",
      "  ca-certificates    pkgs/main/osx-arm64::ca-certificates-2025.2.25-hca03da5_0 \n",
      "  expat              pkgs/main/osx-arm64::expat-2.7.1-h313beb8_0 \n",
      "  libcxx             pkgs/main/osx-arm64::libcxx-14.0.6-h848a8c0_0 \n",
      "  libffi             pkgs/main/osx-arm64::libffi-3.4.4-hca03da5_1 \n",
      "  ncurses            pkgs/main/osx-arm64::ncurses-6.4-h313beb8_0 \n",
      "  openssl            pkgs/main/osx-arm64::openssl-3.0.16-h02f6b3c_0 \n",
      "  pip                pkgs/main/noarch::pip-25.1-pyhc872135_2 \n",
      "  python             pkgs/main/osx-arm64::python-3.12.9-h99e199e_0 \n",
      "  readline           pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 \n",
      "  setuptools         pkgs/main/osx-arm64::setuptools-78.1.1-py312hca03da5_0 \n",
      "  sqlite             pkgs/main/osx-arm64::sqlite-3.45.3-h80987f9_0 \n",
      "  tk                 pkgs/main/osx-arm64::tk-8.6.14-h6ba3021_0 \n",
      "  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0 \n",
      "  wheel              pkgs/main/osx-arm64::wheel-0.45.1-py312hca03da5_0 \n",
      "  xz                 pkgs/main/osx-arm64::xz-5.6.4-h80987f9_1 \n",
      "  zlib               pkgs/main/osx-arm64::zlib-1.2.13-h18a0788_1 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "python-3.12.9        | 13.9 MB   |                                       |   0% \n",
      "setuptools-78.1.1    | 2.2 MB    |                                       |   0% \u001b[A\n",
      "\n",
      "expat-2.7.1          | 156 KB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "wheel-0.45.1         | 148 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "python-3.12.9        | 13.9 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "setuptools-78.1.1    | 2.2 MB    | 2                                     |   1% \u001b[A\n",
      "\n",
      "expat-2.7.1          | 156 KB    | ##############################2       |  82% \u001b[A\u001b[A\n",
      "\n",
      "expat-2.7.1          | 156 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "wheel-0.45.1         | 148 KB    | ###############################9      |  86% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "wheel-0.45.1         | 148 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "setuptools-78.1.1    | 2.2 MB    | ##                                    |   6% \u001b[A\n",
      "\n",
      "python-3.12.9        | 13.9 MB   | 1                                     |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "python-3.12.9        | 13.9 MB   | 7                                     |   2% \u001b[A\u001b[A\u001b[A\n",
      "python-3.12.9        | 13.9 MB   | ##5                                   |   7% \u001b[A\n",
      "python-3.12.9        | 13.9 MB   | ####4                                 |  12% \u001b[A\n",
      "python-3.12.9        | 13.9 MB   | #####4                                |  15% \u001b[A\n",
      "python-3.12.9        | 13.9 MB   | ######3                               |  17% \u001b[A\n",
      "setuptools-78.1.1    | 2.2 MB    | ##################################### | 100% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate iterative-shader-lab\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 24.11.3\n",
      "    latest version: 25.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting openai\n",
      "  Using cached openai-1.82.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Using cached pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Collecting packaging>=20.0 (from matplotlib)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached openai-1.82.1-py3-none-any.whl (720 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl (320 kB)\n",
      "Using cached pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/11.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/11.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/11.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/11.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/11.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/11.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m8.1/11.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m9.2/11.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m11.3/11.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/5.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/5.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m3.7/5.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m4.2/5.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.3-cp312-cp312-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/8.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/8.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/8.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/8.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/8.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/8.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m6.3/8.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m7.6/8.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl (255 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.1-cp312-cp312-macosx_10_13_universal2.whl (2.7 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/2.7 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Downloading pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/3.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: pytz, tzdata, typing-extensions, tqdm, sniffio, six, pyparsing, pillow, packaging, numpy, kiwisolver, jiter, idna, h11, fonttools, distro, cycler, certifi, annotated-types, typing-inspection, python-dateutil, pydantic-core, httpcore, contourpy, anyio, pydantic, pandas, matplotlib, httpx, openai\n",
      "\u001b[?25l\n",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/30\u001b[0m [tqdm]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/30\u001b[0m [pillow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/30\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/30\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/30\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/30\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/30\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/30\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/30\u001b[0m [idna]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/30\u001b[0m [fonttools]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/30\u001b[0m [fonttools]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/30\u001b[0m [fonttools]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m22/30\u001b[0m [httpcore]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m25/30\u001b[0m [pydantic]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m26/30\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m27/30\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m27/30\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m27/30\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m27/30\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m27/30\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m29/30\u001b[0m [openai]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m29/30\u001b[0m [openai]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m29/30\u001b[0m [openai]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30/30\u001b[0m [openai]\n",
      "\u001b[?25h\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 anyio-4.9.0 certifi-2025.4.26 contourpy-1.3.2 cycler-0.12.1 distro-1.9.0 fonttools-4.58.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jiter-0.10.0 kiwisolver-1.4.8 matplotlib-3.10.3 numpy-2.2.6 openai-1.82.1 packaging-25.0 pandas-2.2.3 pillow-11.2.1 pydantic-2.11.5 pydantic-core-2.33.2 pyparsing-3.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 sniffio-1.3.1 tqdm-4.67.1 typing-extensions-4.13.2 typing-inspection-0.4.1 tzdata-2025.2\n",
      "\n",
      "Collecting ipykernel\n",
      "  Using cached ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting appnope (from ipykernel)\n",
      "  Using cached appnope-0.1.4-py2.py3-none-any.whl.metadata (908 bytes)\n",
      "Collecting comm>=0.1.1 (from ipykernel)\n",
      "  Using cached comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting debugpy>=1.6.5 (from ipykernel)\n",
      "  Downloading debugpy-1.8.14-cp312-cp312-macosx_14_0_universal2.whl.metadata (1.3 kB)\n",
      "Collecting ipython>=7.23.1 (from ipykernel)\n",
      "  Downloading ipython-9.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting jupyter-client>=6.1.12 (from ipykernel)\n",
      "  Using cached jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
      "  Using cached jupyter_core-5.8.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting matplotlib-inline>=0.1 (from ipykernel)\n",
      "  Using cached matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting nest-asyncio (from ipykernel)\n",
      "  Using cached nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: packaging in /Users/soham/miniforge3/envs/iterative-shader-lab/lib/python3.12/site-packages (from ipykernel) (25.0)\n",
      "Collecting psutil (from ipykernel)\n",
      "  Using cached psutil-7.0.0-cp36-abi3-macosx_11_0_arm64.whl.metadata (22 kB)\n",
      "Collecting pyzmq>=24 (from ipykernel)\n",
      "  Downloading pyzmq-26.4.0-cp312-cp312-macosx_10_15_universal2.whl.metadata (6.0 kB)\n",
      "Collecting tornado>=6.1 (from ipykernel)\n",
      "  Using cached tornado-6.5.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (2.8 kB)\n",
      "Collecting traitlets>=5.4.0 (from ipykernel)\n",
      "  Using cached traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting decorator (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting ipython-pygments-lexers (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting prompt_toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached prompt_toolkit-3.0.51-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting pygments>=2.4.0 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting stack_data (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting wcwidth (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel)\n",
      "  Using cached wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
      "  Using cached parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/soham/miniforge3/envs/iterative-shader-lab/lib/python3.12/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Collecting platformdirs>=2.5 (from jupyter-core!=5.0.*,>=4.12->ipykernel)\n",
      "  Using cached platformdirs-4.3.8-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
      "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/soham/miniforge3/envs/iterative-shader-lab/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
      "Collecting executing>=1.2.0 (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Using cached executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Using cached asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pure-eval (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Using cached pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Using cached ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
      "Using cached comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Downloading debugpy-1.8.14-cp312-cp312-macosx_14_0_universal2.whl (2.5 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/2.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ipython-9.3.0-py3-none-any.whl (605 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/605.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.3/605.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached prompt_toolkit-3.0.51-py3-none-any.whl (387 kB)\n",
      "Using cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Using cached parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
      "Using cached jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
      "Using cached jupyter_core-5.8.1-py3-none-any.whl (28 kB)\n",
      "Using cached matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
      "Using cached pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "Using cached platformdirs-4.3.8-py3-none-any.whl (18 kB)\n",
      "Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Using cached pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "Downloading pyzmq-26.4.0-cp312-cp312-macosx_10_15_universal2.whl (1.3 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tornado-6.5.1-cp39-abi3-macosx_10_9_universal2.whl (441 kB)\n",
      "Using cached traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "Using cached appnope-0.1.4-py2.py3-none-any.whl (4.3 kB)\n",
      "Using cached decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
      "Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)\n",
      "Using cached nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Using cached psutil-7.0.0-cp36-abi3-macosx_11_0_arm64.whl (239 kB)\n",
      "Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Using cached asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
      "Using cached executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Using cached pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
      "Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Installing collected packages: wcwidth, pure-eval, ptyprocess, traitlets, tornado, pyzmq, pygments, psutil, prompt_toolkit, platformdirs, pexpect, parso, nest-asyncio, executing, decorator, debugpy, asttokens, appnope, stack_data, matplotlib-inline, jupyter-core, jedi, ipython-pygments-lexers, comm, jupyter-client, ipython, ipykernel\n",
      "\u001b[?25l\n",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/27\u001b[0m [tornado]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/27\u001b[0m [pygments]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/27\u001b[0m [pygments]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/27\u001b[0m [pygments]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/27\u001b[0m [psutil]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/27\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/27\u001b[0m [debugpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/27\u001b[0m [debugpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/27\u001b[0m [debugpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m20/27\u001b[0m [jupyter-core]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m21/27\u001b[0m [jedi]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m21/27\u001b[0m [jedi]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m25/27\u001b[0m [ipython]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m26/27\u001b[0m [ipykernel]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/27\u001b[0m [ipykernel]\n",
      "\u001b[?25h\n",
      "\u001b[1A\u001b[2KSuccessfully installed appnope-0.1.4 asttokens-3.0.0 comm-0.2.2 debugpy-1.8.14 decorator-5.2.1 executing-2.2.0 ipykernel-6.29.5 ipython-9.3.0 ipython-pygments-lexers-1.1.1 jedi-0.19.2 jupyter-client-8.6.3 jupyter-core-5.8.1 matplotlib-inline-0.1.7 nest-asyncio-1.6.0 parso-0.8.4 pexpect-4.9.0 platformdirs-4.3.8 prompt_toolkit-3.0.51 psutil-7.0.0 ptyprocess-0.7.0 pure-eval-0.2.3 pygments-2.19.1 pyzmq-26.4.0 stack_data-0.6.3 tornado-6.5.1 traitlets-5.14.3 wcwidth-0.2.13\n",
      "\n",
      "Installed kernelspec iterative-shader-lab in /Users/soham/Library/Jupyter/kernels/iterative-shader-lab\n",
      "\n",
      "Restart the notebook and select the 'Python (iterative-shader-lab)' kernel for all further steps. Restart your IDE if the 'Python (iterative-shader-lab)' kernel does not show up at first.\n"
     ]
    }
   ],
   "source": [
    "# Robust environment setup: forcibly remove the env directory, then create fresh\n",
    "\n",
    "env_name = \"iterative-shader-lab\"\n",
    "env_path = f\"$HOME/miniforge3/envs/{env_name}\"\n",
    "\n",
    "# Remove the directory if it exists (even if not a valid conda env)\n",
    "!rm -rf {env_path}\n",
    "\n",
    "# Now create the environment from scratch\n",
    "!conda create -y -n {env_name} python=3.12\n",
    "\n",
    "# Install dependencies\n",
    "!conda install -y -n {env_name} pip\n",
    "!conda run -n {env_name} pip install openai pandas tqdm numpy matplotlib\n",
    "\n",
    "# Install jupyter in the env and register the kernel\n",
    "!conda run -n {env_name} pip install ipykernel\n",
    "!conda run -n {env_name} python -m ipykernel install --user --name {env_name} --display-name \"Python ({env_name})\"\n",
    "\n",
    "print(\"Restart the notebook and select the 'Python (iterative-shader-lab)' kernel for all further steps. Restart your IDE if the 'Python (iterative-shader-lab)' kernel does not show up at first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9b9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Set up warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab53cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository already exists. Ensuring we’re on master branch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Already on 'master'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your branch is up to date with 'origin/master'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/pandahop/shaderCrawl\n",
      " * branch            master     -> FETCH_HEAD\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "Checked out master and pulled latest.\n",
      "Extraction complete! CSV file saved to: shaders_extracted.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Clone the repository’s master branch if it doesn't already exist\n",
    "repo_url = \"https://github.com/pandahop/shaderCrawl.git\"\n",
    "clone_dir = \"shaderCrawl\"\n",
    "\n",
    "if not os.path.isdir(clone_dir):\n",
    "    print(f\"Cloning master branch from {repo_url}...\")\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"clone\", \"--single-branch\", \"--branch\", \"master\", repo_url, clone_dir],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        print(\"Error cloning repository:\")\n",
    "        print(result.stderr)\n",
    "        raise RuntimeError(\"Failed to clone repository\")\n",
    "    else:\n",
    "        print(\"Repository cloned (master branch).\")\n",
    "else:\n",
    "    print(\"Repository already exists. Ensuring we’re on master branch...\")\n",
    "    # If it already exists, switch to master\n",
    "    subprocess.run([\"git\", \"-C\", clone_dir, \"fetch\"], check=True)\n",
    "    subprocess.run([\"git\", \"-C\", clone_dir, \"checkout\", \"master\"], check=True)\n",
    "    subprocess.run([\"git\", \"-C\", clone_dir, \"pull\", \"origin\", \"master\"], check=True)\n",
    "    print(\"Checked out master and pulled latest.\")\n",
    "\n",
    "# 2. Define paths\n",
    "input_dir = os.path.join(clone_dir, \"raw_pages\")\n",
    "output_csv = \"shaders_extracted.csv\"\n",
    "\n",
    "# 3. Regular expression to locate the gShaders array in each HTML file\n",
    "gshaders_pattern = re.compile(\n",
    "    r\"gShaders\\s*=\\s*(\\[\\s*\\{.*?\\}\\s*\\]);\",\n",
    "    flags=re.DOTALL\n",
    ")\n",
    "\n",
    "# 4. Container for rows to write to CSV\n",
    "rows = []\n",
    "\n",
    "# 5. Walk through the directory and process each HTML file\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".html\"):\n",
    "            filepath = os.path.join(root, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                content = f.read()\n",
    "\n",
    "                # Search for the gShaders JSON array\n",
    "                match = gshaders_pattern.search(content)\n",
    "                if not match:\n",
    "                    continue\n",
    "\n",
    "                json_text = match.group(1)\n",
    "                try:\n",
    "                    shaders_list = json.loads(json_text)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "                # Extract description and code from each render pass\n",
    "                for shader in shaders_list:\n",
    "                    renderpasses = shader.get(\"renderpass\", [])\n",
    "                    top_description = shader.get(\"info\", {}).get(\"description\", \"\")\n",
    "\n",
    "                    for rp in renderpasses:\n",
    "                        desc = rp.get(\"description\", \"\").strip() or top_description.strip()\n",
    "                        code = rp.get(\"code\", \"\").strip()\n",
    "\n",
    "                        if code:\n",
    "                            rows.append([desc, code])\n",
    "\n",
    "# 6. Write all collected rows into the CSV\n",
    "with open(output_csv, \"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"description\", \"code\"])\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Extraction complete! CSV file saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50983ed",
   "metadata": {},
   "source": [
    "## 2. Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410ee870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goofing some more...</td>\n",
       "      <td>#define FARCLIP    35.0\\n\\n#define MARCHSTEPS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            description                                               code\n",
       "0  goofing some more...  #define FARCLIP    35.0\\n\\n#define MARCHSTEPS ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shader code preview (first 500 chars):\n",
      "#define FARCLIP    35.0\n",
      "\n",
      "#define MARCHSTEPS 60\n",
      "#define AOSTEPS    8\n",
      "#define SHSTEPS    10\n",
      "#define SHPOWER    3.0\n",
      "\n",
      "#define PI         3.14\n",
      "#define PI2        PI*0.5    \n",
      "\n",
      "#define AMBCOL     vec3(1.0,1.0,1.0)\n",
      "#define BACCOL     vec3(1.0,1.0,1.0)\n",
      "#define DIFCOL     vec3(1.0,1.0,1.0)\n",
      "\n",
      "#define MAT1       1.0\n",
      "\n",
      "#define FOV 1.0\n",
      "\n",
      "\n",
      "/***********************************************/\n",
      "float rbox(vec3 p, vec3 s, float r) {\t\n",
      "    return length(max(abs(p)-s+vec3(r),0.0))-r;\n",
      "}\n",
      "float torus(vec3 p, vec2 t) {\n",
      "    vec2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your new CSV\n",
    "csv_path = './shaders_extracted.csv'\n",
    "\n",
    "# Load the CSV (change nrows=1 to load more if you want)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Pick a row to explore (e.g., the first row)\n",
    "row_idx = 0\n",
    "row = df.iloc[row_idx]\n",
    "\n",
    "# Display the row as a DataFrame for readability\n",
    "display(pd.DataFrame([row]))\n",
    "\n",
    "# If you want to print the code field\n",
    "code_col = next((col for col in df.columns if 'code' in col.lower()), None)\n",
    "if code_col:\n",
    "    print(f\"\\nShader code preview (first 500 chars):\\n{row[code_col][:500]}\")\n",
    "else:\n",
    "    print(\"No code column found in this CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d160ae31",
   "metadata": {},
   "source": [
    "## 3. Preprocess and Convert to JSONL Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef8cd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in DataFrame: ['description', 'code']\n",
      "First row of DataFrame:\n",
      " description                                 goofing some more...\n",
      "code           #define FARCLIP    35.0\\n\\n#define MARCHSTEPS ...\n",
      "Name: 0, dtype: object\n",
      "Creating training set with 1370 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1370/1370 [00:00<00:00, 8917.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written: 1370, Skipped: 0\n",
      "Creating validation set with 152 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:00<00:00, 4946.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written: 152, Skipped: 0\n",
      "Training data saved to: ./data/shadertoy-1000_train.jsonl\n",
      "Validation data saved to: ./data/shadertoy-1000_val.jsonl\n",
      "\n",
      "Sample from ./data/shadertoy-1000_train.jsonl:\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a GLSL expert. Given a shader description, produce complete GLSL (vertex+fragment) shader code.\"}, {\"role\": \"user\", \"content\": \"Description: This is supposed to be a clone of the Windows \\\"Mystify\\\" screensaver. I guess I got close but you can definitely tell that it's not as good as the original.\\n\\nMusic: https://soundcloud.com/3kliksphilip/under-the-pizza\"}, {\"role\": \"assistant\", \"content\": \"//Make random values more random\\nfloat randSeed = 0.;\\n\\n#define FLOORI(x) float(int(floor(x)))\\n\\n/* old function\\nfloat smoothRand(float interval, float seed) {\\n    float next = rand(1.+floor(iTime/interval)+seed);\\n    float curr = rand(floor(iTime/interval)+seed);\\n    randSeed++;\\n    return mix(curr, next, fract(iTime/interval));\\n}\\n*/\\n\\nfloat smoothRand(float interval, float seed) {\\n    float next = rand(.000001*FLOORI(1000000.*(1.+FLOORI(iTime/interval)+seed+randSeed)));\\n    float curr = rand(.000001*FLOORI(1000000.*(FLOORI(iTime/interval)+seed+randSeed)));\\n    randSeed++;\\n    return mix(curr, next, fract(iTime/interval));\\n}\\n\\nfloat f(vec2 point) {\\n\\treturn sin(point.x*2.+iTime*1.275)+point.y;   \\n}\\n\\nvec2 grad( in vec2 x )\\n{\\n    vec2 h = vec2( 0.01, 0.0 );\\n    return vec2( f(x+h.xy) - f(x-h.xy),\\n                 f(x+h.yx) - f(x-h.yx) )/(2.0*h.x);\\n}\\n\\n//https://iquilezles.org/articles/distance\\nfloat color( in vec2 point, in int lineWidth, in vec3 iResolution)\\n{\\n    float v = f( point );\\n    vec2  g = grad( point );\\n    float de = abs(v)/length(g);\\n    float normalizedLineRadius = (float(max(5,lineWidth)) / iResolution.y) / 2.;\\n    float edgeWidth = 1. / iResolution.y;\\n    if(normalizedLineRadius<1./iResolution.x)\\n        return 0.;\\n    float eps = max(1./iResolution.x, 1./iResolution.y)*normalizedLineRadius;\\n    return 1.-clamp(smoothstep( 0., normalizedLineRadius, de ), 0., 1.);\\n}\\n\\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\\n{    \\n    //init\\n    vec2 uv = fragCoord/iResolution.xy;\\n    vec2 point = vec2( fragCoord.xy - 0.5*iResolution.xy );\\n\\tpoint = 2.0 * point.xy / iResolution.y;\\n    float x,y,z = 0.;\\n    fragColor = vec4(0);\\n \\n    //Limit \\\"frames\\\" for uniterrupted animation\\n    float frameRatio = floor(iFrameRate/ 30.);\\n    if(mod(float(iFrame), frameRatio) != 0.) {\\n        float decay = sinNorm(iTime*0.789)*0.5+0.1;\\n        fragColor += texture(iChannel0, uv) * (1.-(1./(decay*iFrameRate)));\\n\\n        //Clamp to prevent overexposure\\n        fragColor.r = clamp(fragColor.r, 0., 1.);\\n        fragColor.g = clamp(fragColor.g, 0., 1.);\\n        fragColor.b = clamp(fragColor.b, 0., 1.);\\n        return;   \\n    }\\n    \\n    //Scaling\\n    const float maxZoom = 1.5;\\n    const float minZoom = 0.5;\\n    const float changeInterval = 2.;\\n    float nextZ = rand(1.+floor(iTime/changeInterval));\\n    float currZ = rand(floor(iTime/changeInterval));\\n    z=minZoom+(maxZoom-minZoom)*mix(currZ, nextZ, fract(iTime/changeInterval));\\n    point/=vec2(z);\\n    \\n    //Rotation\\n    float rot = smoothRand(0.5,354.856)*PI;\\n    point=vec2(cos(rot)*point.x+sin(rot)*point.y, -sin(rot)*point.x+cos(rot)*point.y);\\n    \\n    //Translation\\n    point.x+=smoothRand(1.,842.546)*2.-1.;\\n    //No need to translate y here, bc y is set by the function in \\\"f(point)\\\" and the rotation.\\n    \\n    //Line\\n    const float minLength = 0.25;\\n    const float maxLength=0.5;\\n    float lineLength=minLength+smoothRand(4.,0.846)*(maxLength-minLength)+minLength;\\n    float linePoint = (point.x+lineLength/2.) / lineLength;\\n    //\\t\\t\\t\\tclamp - make sure the value is in bounds\\n    //\\t\\t\\t\\t\\t\\t  smoothVal - make the line thinner at the ends\\n    int lineWidth = int(clamp(floor(smoothVal(linePoint*100., 100.)*iResolution.x*0.025*z), 2., floor(iResolution.x*0.025*z)));//max(3,int((iResolution.x*0.1)*  pow((point.x*(1./lineLength)),3.)  ));\\n    if(point.x >= -lineLength / 2. && point.x <= lineLength / 2.) { //Only show a small segment\\n    \\tfragColor+=color(vec2(point.x,point.y), lineWidth, iResolution);//line(lineWidth, vec2(x,y), point, iResolution);\\n\\t}\\t\\n    /*if(point.x-x<0.005) {\\n        fragColor = vec4(1.);\\n    }\\n    if(point.y-y<0.005) {\\n\\t\\t//fragColor = vec4(1.);\\n    }*/\\n    //Color\\n    fragColor.rgb*=hsv2rgb(vec3(fract(iTime/7.), sinNorm(iTime*rand(135.54))*0.4+0.6,1.));\\n    fragColor.rgb+=pow((fragColor.r+fragColor.g+fragColor.b)/3.+0.25,3.)-pow(0.25,3.);\\n    \\n    //Fade\\n    float decay = sinNorm(iTime*0.789)*0.5+0.25;\\n    fragColor += texture(iChannel0, uv) * (1.-(1./(decay*iFrameRate)));\\n    \\n    //Clamp to prevent overexposure\\n    fragColor.r = clamp(fragColor.r, 0., 2.);\\n    fragColor.g = clamp(fragColor.g, 0., 2.);\\n    fragColor.b = clamp(fragColor.b, 0., 2.);\\n    fragColor.a = 0.;\\n}\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a GLSL expert. Given a shader description, produce complete GLSL (vertex+fragment) shader code.\"}, {\"role\": \"user\", \"content\": \"Description: My first attempt at volumetric clouds based on Horizon Zero Dawn clouds. Most of the logic for the clouds is in buffer C. Use the mouse to drag around the sun for different times of the day, or click at the bottom for an automated day/night cycle.\"}, {\"role\": \"assistant\", \"content\": \"/**\\n  Buffer C draws the clouds in the sky. The texture from buffer A is used to model the\\n  clouds in the ray march and the light march loops. Just like buffer B, only 1 out 16\\n  pixels are processed per frame and the rest are reprojected. If anyone's interested,\\n  I've compiled a useful list of resources for rendering realtime volumetric clouds\\n  here: https://gist.github.com/pxv8270/e3904c49cbd8ff52cb53d95ceda3980e\\n*/\\n\\nconst vec3 noiseKernel[6u] = vec3[] \\n(\\n\\tvec3( .38051305,  .92453449, -.02111345),\\n\\tvec3(-.50625799, -.03590792, -.86163418),\\n\\tvec3(-.32509218, -.94557439,  .01428793),\\n\\tvec3( .09026238, -.27376545,  .95755165),\\n\\tvec3( .28128598,  .42443639, -.86065785),\\n\\tvec3(-.16852403,  .14748697,  .97460106)\\n);\\n\\n//-------------------------------------------------------------------------------------\\n// Clouds modeling\\n//-------------------------------------------------------------------------------------\\n\\nfloat raySphereIntersect(Ray ray, float radius)\\n{\\n    // note to future me: don't need \\\"a\\\" bcuz rd is normalized and dot(rd, rd) = 1\\n \\tfloat b = 2. * dot(ray.origin, ray.direction);\\n    float c = dot(ray.origin, ray.origin) - radius * radius;\\n    float d = sqrt(b * b - 4. * c);\\n    return (-b + d) * .5;\\n}\\n\\nfloat cloudGradient(float h)\\n{\\n    return smoothstep(0., .05, h) * smoothstep(1.25, .5, h);\\n}\\n\\nfloat cloudHeightFract(float p)\\n{\\n\\treturn (p - EARTH_RADIUS - CLOUD_BOTTOM) / (CLOUD_TOP - CLOUD_BOTTOM);\\n}\\n\\nfloat cloudBase(vec3 p, float y)\\n{\\n    vec3 noise = textureLod(iChannel2, (p.xz - (WIND_DIR.xz * iTime * WIND_SPEED))\\n                            * CLOUD_BASE_FREQ, 0.).rgb;\\n    float n = y * y * noise.b + pow(1. - y, 12.);\\n    float cloud = remap01(noise.r - n, noise.g - 1., 1.);\\n    return cloud;\\n}\\n\\nfloat cloudDetail(vec3 p, float c, float y)\\n{\\n    p -= WIND_DIR * 3. * iTime * WIND_SPEED;\\n    // this is super expensive :(\\n    float hf = worleyFbm(p, CLOUD_DETAIL_FREQ, false) * .625 +\\n        \\t   worleyFbm(p, CLOUD_DETAIL_FREQ*2., false) * .25 +\\n        \\t   worleyFbm(p, CLOUD_DETAIL_FREQ*4., false) * .125;\\n    hf = mix(hf, 1. - hf, y * 4.);\\n    return remap01(c, hf * .5, 1.);\\n}\\n\\nfloat getCloudDensity(vec3 p, float y, bool detail)\\n{\\n    p.xz -= WIND_DIR.xz * y * CLOUD_TOP_OFFSET;\\n    float d = cloudBase(p, y);\\n    d = remap01(d, CLOUD_COVERAGE, 1.) * (CLOUD_COVERAGE);\\n    d *= cloudGradient(y);\\n    bool cloudDetailTest = (d > 0. && d < .3) && detail; \\n    return ((cloudDetailTest) ? cloudDetail(p, d, y) : d);\\n}\\n\\n//-------------------------------------------------------------------------------------\\n// Clouds lighting\\n//-------------------------------------------------------------------------------------\\n\\nfloat henyeyGreenstein( float sunDot, float g) {\\n\\tfloat g2 = g * g;\\n\\treturn (.25 / PI) * ((1. - g2) / pow( 1. + g2 - 2. * g * sunDot, 1.5));\\n}\\n\\nfloat marchToLight(vec3 p, vec3 sunDir, float sunDot, float scatterHeight)\\n{\\n    float lightRayStepSize = 11.;\\n\\tvec3 lightRayDir = sunDir * lightRayStepSize;\\n    vec3 lightRayDist = lightRayDir * .5;\\n    float coneSpread = length(lightRayDir);\\n    float totalDensity = 0.;\\n    for(int i = 0; i < CLOUD_LIGHT_STEPS; ++i)\\n    {\\n        // cone sampling as explained in GPU Pro 7 article\\n     \\tvec3 cp = p + lightRayDist + coneSpread * noiseKernel[i] * float(i);\\n        float y = cloudHeightFract(length(p));\\n        if (y > .95 || totalDensity > .95) break; // early exit\\n        totalDensity += getCloudDensity(cp, y, false) * lightRayStepSize;\\n        lightRayDist += lightRayDir;\\n    }\\n    \\n    return 32. * exp(-totalDensity * mix(CLOUD_ABSORPTION_BOTTOM,\\n\\t\\t\\t\\tCLOUD_ABSORPTION_TOP, scatterHeight)) * (1. - exp(-totalDensity * 2.));\\n}\\n\\n//-------------------------------------------------------------------------------------\\n\\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\\n{\\n    vec2 st = fragCoord / iResolution.xy;\\n    vec2 uv = (2. * fragCoord - iResolution.xy) / iResolution.y;\\n    vec2 mouse = (2. * iMouse.xy - iResolution.xy) / iResolution.y;\\n    float terrainDist = texelFetch(iChannel0, ivec2(fragCoord), 0).w;\\n    vec4 prevCol = textureLod(iChannel1, st, 0.);\\n    vec4 col = vec4(0.);\\n    \\n    bool updatePixel = writeToPixel(fragCoord, iFrame);\\n    \\n    if (updatePixel) // only draw 1/16th resolution per frame\\n    {\\n        \\n        Ray ray = getCameraRay(uv, iTime);\\n        vec3 sun = getSun(mouse, iTime);\\n        // clouds don't get blindingly bright with sun at zenith\\n        sun.z = clamp(sun.z, 0., .8);\\n        vec3 sunDir = normalize(vec3(sun.x, sun.z, -1.));\\n        float sunDot = max(0., dot(ray.direction, sunDir));\\n        float sunHeight = smoothstep(.01, .1, sun.z + .025);\\n        \\n        if (terrainDist > CAMERA_FAR)\\n        {\\n\\n            // clouds\\n            ray.origin.y = EARTH_RADIUS;\\n            float start = raySphereIntersect(ray, EARTH_RADIUS + CLOUD_BOTTOM);\\n            float end = raySphereIntersect(ray, EARTH_RADIUS + CLOUD_TOP);\\n            float cameraRayDist = start;\\n            float cameraRayStepSize = (end - start) / float(CLOUD_STEPS);\\n            \\n            // blue noise offset\\n            cameraRayDist += cameraRayStepSize * texelFetch(iChannel3,\\n\\t\\t\\t\\t\\t\\t\\t(ivec2(fragCoord) + iFrame * ivec2(113, 127)) & 1023, 0).r;\\n            vec3 skyCol = atmosphericScattering(vec2(0.15, 0.05),\\n                                vec2(.5, sun.y*.5+.25), false);\\n            skyCol.r *= 1.1;\\n\\t\\t\\tskyCol = SAT(pow(skyCol * 2.1, vec3(4.2)));\\n            float sunScatterHeight = smoothstep(.15, .4, sun.z);\\n            float hgPhase = mix(henyeyGreenstein(sunDot, .4),\\n                                henyeyGreenstein(sunDot, -.1), .5);\\n            // sunrise/sunset hack\\n            hgPhase = max(hgPhase, 1.6 * henyeyGreenstein(sqrt(sunDot),\\n\\t\\t\\t\\t\\t\\t\\tSAT(.8 - sunScatterHeight)));\\n            // shitty night time hack\\n            hgPhase = mix(pow(sunDot, .25), hgPhase, sunHeight);\\n            \\n            vec4 intScatterTrans = vec4(0., 0., 0., 1.);\\n            vec3 ambient = vec3(0.);\\n            for (int i = 0; i < CLOUD_STEPS; ++i)\\n            {\\n                vec3 p = ray.origin + cameraRayDist * ray.direction;\\n                float heightFract = cloudHeightFract(length(p));\\n                float density = getCloudDensity(p, heightFract, true);\\n                if (density > 0.)\\n                {\\n                    ambient = mix(CLOUDS_AMBIENT_BOTTOM, CLOUDS_AMBIENT_TOP, \\n                                  \\theightFract);\\n\\t\\t\\t\\t\\t\\n                    // cloud illumination\\n                    vec3 luminance = (ambient * SAT(pow(sun.z + .04, 1.4))\\n\\t\\t\\t\\t\\t\\t+ skyCol * .125 + (sunHeight * skyCol + vec3(.0075, .015, .03))\\n\\t\\t\\t\\t\\t\\t* SUN_COLOR * hgPhase\\n\\t\\t\\t\\t\\t\\t* marchToLight(p, sunDir, sunDot, sunScatterHeight)) * density;\\n\\n                    // improved scatter integral by S\\u00e9bastien Hillaire\\n                    float transmittance = exp(-density * cameraRayStepSize);\\n                    vec3 integScatter = (luminance - luminance * transmittance)\\n                        * (1. / density);\\n                    intScatterTrans.rgb += intScatterTrans.a * integScatter; \\n                    intScatterTrans.a *= transmittance;\\n\\n                }\\n\\n                if (intScatterTrans.a < .05)\\n                    break;\\n                cameraRayDist += cameraRayStepSize;\\n            }\\n\\n            // blend clouds with sky at a distance near the horizon (again super hacky)\\n            float fogMask = 1. - exp(-smoothstep(.15, 0., ray.direction.y) * 2.);\\n            vec3 fogCol = atmosphericScattering(uv * .5 + .2, sun.xy * .5 + .2, false);\\n            intScatterTrans.rgb = mix(intScatterTrans.rgb,\\n                                      fogCol * sunHeight, fogMask);\\n            intScatterTrans.a = mix(intScatterTrans.a, 0., fogMask);\\n\\n            col = vec4(max(vec3(intScatterTrans.rgb), 0.), intScatterTrans.a);\\n            \\n            //temporal reprojection\\n    \\t\\tcol = mix(prevCol, col, .5);\\n        }\\n    }\\n    else\\n    {\\n\\t\\tcol = prevCol;\\n    }\\n    \\n    fragColor = col;\\n}\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a GLSL expert. Given a shader description, produce complete GLSL (vertex+fragment) shader code.\"}, {\"role\": \"user\", \"content\": \"Description: Zsa Zsa Gabor, rendered with 128 Gabor functions. \\n\\nMore technical details at [url]https://mzucker.github.io/2016/08/01/gabor-2.html[/url]\"}, {\"role\": \"assistant\", \"content\": \"/* Gabor^2, by mattz.\\n   License: Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License.\\n\\n   Goal: render the face of Zsa Zsa Gabor using the basis function proposed by Dennis Gabor.\\n\\n   Original image: http://www.instyle.com/celebrity/gallery-vintage-photos-zsa-zsa-gabor\\n   Gabor filter: https://en.wikipedia.org/wiki/Gabor_filter\\n   More about Dennis: https://en.wikipedia.org/wiki/Dennis_Gabor\\n   More about Zsa Zsa: https://en.wikipedia.org/wiki/Zsa_Zsa_Gabor\\n\\n   Fitting these 128 basis functions took several days (!) on my Macbook Pro (CPU only, no GPU).\\n\\n   See https://mzucker.github.io/2016/08/01/gabor-2.html for more details.\\n\\n   There was probably a faster way of doing it.\\n\\n*/\\n\\nconst vec4 scl = vec4(0.00391389432485, 0.0122958616579, 0.00782778864971, 0.00391389432485);\\n\\nfloat cnt = 0.0;\\n\\nfloat gabor(vec2 p, vec4 q) {\\n    \\n    // Here we decode the vec4 q into 8 individual parameters:\\n    //\\n    //   q0 = (x, a, l, s)\\n    //   q1 = (y, p, t, h)\\n    //\\n    // with parameters given by\\n    //\\n    //   x: function center x coordinate\\n    //   y: function center y coordinate\\n    //   a: Gabor function spatial angle/orientation\\n    //   p: Gabor function phase offset\\n    //   l: Spatial wavelength\\n    //   s: Filter width perpendicular to sinusoidal component \\n    //   t: Filter width parallel to sinusoidal component\\n    //   h: Amplitude\\n    //\\n    vec4 q0 = floor(q*0.001953125)*scl;\\n    vec4 q1 = mod(q, 512.0)*scl;     \\n            \\n    float cr = cos(q0.y);\\n    float sr = sin(q0.y);\\n        \\n    vec2 st = vec2(q0.w, q1.z);\\n\\n    // Rotate and translate point\\n    p = mat2(cr, -sr, sr, cr) * (p - vec2(q0.x, q1.x));    \\n        \\t\\n    // Handle appearing at the start of filter\\n    q1.w *= smoothstep(cnt, cnt+0.5, iTime*32.0);\\n    ++cnt;\\n\\n    // amplitude * gaussian * sinusoid\\n    return q1.w * exp(dot(vec2(-0.5), p*p/(st*st))) * cos(p.x*6.2831853/q0.z+q1.y);\\n    \\n}\\n\\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord ) {\\n        \\n\\tvec2 p = (fragCoord.xy - 0.5*iResolution.xy) * 1.2 / iResolution.y;\\n    p.y = -p.y;\\n    p += 1.0;\\n    \\n    float k = 0.0;\\n\\n    k += gabor(p, vec4(138752.,150667.,261821.,93447.));\\n    k += gabor(p, vec4(226145.,131739.,77399.,33587.));\\n    k += gabor(p, vec4(63748.,240999.,76311.,23447.));\\n    k += gabor(p, vec4(150173.,43128.,50197.,22225.));\\n    k += gabor(p, vec4(116939.,73605.,57898.,7679.));\\n    k += gabor(p, vec4(51.,227699.,261732.,68280.));\\n    k += gabor(p, vec4(50687.,129029.,38508.,17512.));\\n    k += gabor(p, vec4(117913.,117281.,32319.,32324.));\\n    k += gabor(p, vec4(96910.,174542.,26669.,26719.));\\n    k += gabor(p, vec4(117600.,70656.,17940.,16010.));\\n    k += gabor(p, vec4(108871.,58049.,51302.,47654.));\\n    k += gabor(p, vec4(167609.,91829.,28204.,28213.));\\n    k += gabor(p, vec4(90467.,245116.,36375.,7167.));\\n    k += gabor(p, vec4(82369.,164365.,30329.,30267.));\\n    k += gabor(p, vec4(225089.,129997.,6684.,4095.));\\n    k += gabor(p, vec4(205005.,127709.,12815.,12982.));\\n    k += gabor(p, vec4(175993.,130600.,23563.,11978.));\\n    k += gabor(p, vec4(30069.,10115.,23055.,2559.));\\n    k += gabor(p, vec4(137435.,215947.,14350.,14446.));\\n    k += gabor(p, vec4(226165.,115949.,47659.,18501.));\\n    k += gabor(p, vec4(71368.,123529.,23052.,12515.));\\n    k += gabor(p, vec4(102042.,215281.,7176.,2857.));\\n    k += gabor(p, vec4(68218.,214988.,25632.,25694.));\\n    k += gabor(p, vec4(115954.,5782.,40979.,2930.));\\n    k += gabor(p, vec4(123338.,133796.,77334.,23039.));\\n    k += gabor(p, vec4(203913.,117.,38944.,33329.));\\n    k += gabor(p, vec4(227964.,132231.,189988.,12174.));\\n    k += gabor(p, vec4(63649.,26205.,10763.,10940.));\\n    k += gabor(p, vec4(97912.,45398.,12810.,1739.));\\n    k += gabor(p, vec4(106690.,166508.,16387.,2559.));\\n    k += gabor(p, vec4(54873.,146884.,58426.,58402.));\\n    k += gabor(p, vec4(178346.,81559.,14341.,2559.));\\n    k += gabor(p, vec4(185649.,144747.,16391.,1535.));\\n    k += gabor(p, vec4(120084.,199932.,10249.,4361.));\\n    k += gabor(p, vec4(100115.,63452.,18981.,18968.));\\n    k += gabor(p, vec4(180941.,96255.,18962.,18500.));\\n    k += gabor(p, vec4(63718.,126911.,34827.,11485.));\\n    k += gabor(p, vec4(61203.,92616.,23582.,18498.));\\n    k += gabor(p, vec4(172754.,196259.,16390.,6559.));\\n    k += gabor(p, vec4(177015.,230484.,25173.,24616.));\\n    k += gabor(p, vec4(157875.,57828.,4618.,4837.));\\n    k += gabor(p, vec4(105263.,51711.,17997.,17939.));\\n    k += gabor(p, vec4(167049.,203925.,24612.,15906.));\\n    k += gabor(p, vec4(144087.,67302.,14874.,14931.));\\n    k += gabor(p, vec4(150391.,43665.,53778.,4032.));\\n    k += gabor(p, vec4(207569.,22528.,15906.,8795.));\\n    k += gabor(p, vec4(137039.,173193.,27653.,2559.));\\n    k += gabor(p, vec4(206047.,207878.,18958.,7797.));\\n    k += gabor(p, vec4(228200.,131090.,5678.,5799.));\\n    k += gabor(p, vec4(113976.,64789.,7689.,6396.));\\n    k += gabor(p, vec4(83114.,62975.,15903.,13894.));\\n    k += gabor(p, vec4(117517.,132261.,12814.,11335.));\\n    k += gabor(p, vec4(149162.,180114.,75786.,4971.));\\n    k += gabor(p, vec4(219290.,259568.,7701.,3469.));\\n    k += gabor(p, vec4(9233.,249362.,21102.,14898.));\\n    k += gabor(p, vec4(12348.,105446.,15436.,10340.));\\n    k += gabor(p, vec4(147344.,213862.,14917.,14877.));\\n    k += gabor(p, vec4(205125.,182142.,82962.,5368.));\\n    k += gabor(p, vec4(161978.,187391.,5644.,5760.));\\n    k += gabor(p, vec4(88210.,61601.,16388.,2047.));\\n    k += gabor(p, vec4(78650.,114304.,15885.,2047.));\\n    k += gabor(p, vec4(115065.,71687.,15886.,7819.));\\n    k += gabor(p, vec4(93059.,235007.,12814.,10350.));\\n    k += gabor(p, vec4(196884.,27136.,12303.,5338.));\\n    k += gabor(p, vec4(171860.,258580.,24079.,15418.));\\n    k += gabor(p, vec4(183655.,243097.,21510.,2559.));\\n    k += gabor(p, vec4(191277.,128656.,12293.,3583.));\\n    k += gabor(p, vec4(157553.,43644.,22026.,2014.));\\n    k += gabor(p, vec4(76369.,27135.,14370.,10903.));\\n    k += gabor(p, vec4(194876.,76800.,7683.,3357.));\\n    k += gabor(p, vec4(150218.,45734.,3076.,2201.));\\n    k += gabor(p, vec4(230711.,259797.,3613.,3813.));\\n    k += gabor(p, vec4(143536.,47519.,32771.,2435.));\\n    k += gabor(p, vec4(73356.,64451.,16409.,6741.));\\n    k += gabor(p, vec4(228480.,129023.,2581.,2683.));\\n    k += gabor(p, vec4(193434.,117680.,17930.,4095.));\\n    k += gabor(p, vec4(71419.,126168.,4112.,2190.));\\n    k += gabor(p, vec4(94887.,188293.,41993.,4095.));\\n    k += gabor(p, vec4(221843.,258986.,4634.,4788.));\\n    k += gabor(p, vec4(254463.,215580.,25241.,24612.));\\n    k += gabor(p, vec4(36217.,144383.,23056.,9348.));\\n    k += gabor(p, vec4(110799.,84416.,10835.,10767.));\\n    k += gabor(p, vec4(92864.,216574.,22019.,2047.));\\n    k += gabor(p, vec4(21781.,110203.,209421.,13785.));\\n    k += gabor(p, vec4(42162.,246368.,12299.,11902.));\\n    k += gabor(p, vec4(178010.,28532.,27661.,2559.));\\n    k += gabor(p, vec4(75445.,144528.,29199.,3583.));\\n    k += gabor(p, vec4(66694.,194418.,24581.,5119.));\\n    k += gabor(p, vec4(230325.,1590.,13360.,7865.));\\n    k += gabor(p, vec4(92351.,97505.,5635.,2956.));\\n    k += gabor(p, vec4(75010.,124415.,12335.,7234.));\\n    k += gabor(p, vec4(97906.,169101.,54817.,5119.));\\n    k += gabor(p, vec4(106161.,83324.,3589.,2713.));\\n    k += gabor(p, vec4(113835.,235844.,9225.,3708.));\\n    k += gabor(p, vec4(208627.,241050.,10768.,10482.));\\n    k += gabor(p, vec4(168661.,177062.,8706.,2559.));\\n    k += gabor(p, vec4(181467.,127455.,17938.,7786.));\\n    k += gabor(p, vec4(197869.,146292.,28173.,2559.));\\n    k += gabor(p, vec4(117558.,187903.,9736.,3285.));\\n    k += gabor(p, vec4(222982.,25466.,20483.,3071.));\\n    k += gabor(p, vec4(235759.,128791.,8256.,8275.));\\n    k += gabor(p, vec4(29057.,162597.,13888.,8215.));\\n    k += gabor(p, vec4(190357.,256223.,13345.,4260.));\\n    k += gabor(p, vec4(119069.,231138.,8715.,6737.));\\n    k += gabor(p, vec4(170876.,121183.,16906.,10329.));\\n    k += gabor(p, vec4(154320.,242687.,6147.,3239.));\\n    k += gabor(p, vec4(115981.,136703.,12808.,6807.));\\n    k += gabor(p, vec4(120081.,193574.,9734.,2257.));\\n    k += gabor(p, vec4(134533.,188242.,26635.,1665.));\\n    k += gabor(p, vec4(138043.,77525.,7686.,2271.));\\n    k += gabor(p, vec4(130835.,198313.,5122.,1988.));\\n    k += gabor(p, vec4(102193.,56831.,6148.,1687.));\\n    k += gabor(p, vec4(111902.,152880.,20531.,7206.));\\n    k += gabor(p, vec4(174288.,78489.,3075.,3151.));\\n    k += gabor(p, vec4(96671.,127941.,6669.,4272.));\\n    k += gabor(p, vec4(210122.,17650.,13840.,13455.));\\n    k += gabor(p, vec4(203063.,194037.,33825.,33307.));\\n    k += gabor(p, vec4(149725.,95402.,11796.,11817.));\\n    k += gabor(p, vec4(66297.,11373.,17922.,2559.));\\n    k += gabor(p, vec4(179440.,110836.,5633.,1319.));\\n    k += gabor(p, vec4(43681.,262028.,25090.,3071.));\\n    k += gabor(p, vec4(101192.,233983.,8200.,8304.));\\n    k += gabor(p, vec4(202536.,155036.,12293.,2559.));\\n    k += gabor(p, vec4(197017.,107008.,8719.,2338.));\\n    k += gabor(p, vec4(163006.,200413.,3076.,2203.));\\n    k += gabor(p, vec4(122700.,196047.,6149.,5280.));\\n    k += gabor(p, vec4(138048.,193231.,17925.,1322.));\\n    k += gabor(p, vec4(205566.,243712.,10761.,9529.));   \\n\\n    // Don't add speckles in preview!\\n    if (iResolution.y >= 200.0) {\\n        // borrowed Dave Hoskins' hash from https://www.shadertoy.com/view/4djSRW\\n        p = fract(p * vec2(443.8975,397.2973));\\n        p += dot(p.xy, p.yx+19.19);\\n        k += 0.12 * (fract(p.x * p.y)*2.0 - 1.0);\\n    }\\n    \\n    fragColor.xyz = vec3(0.5*k + 0.5);                 \\n\\t\\n}\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a GLSL expert. Given a shader description, produce complete GLSL (vertex+fragment) shader code.\"}, {\"role\": \"user\", \"content\": \"Description: It's really cool that you can represent a fluid as just up down left right energy\"}, {\"role\": \"assistant\", \"content\": \"// LENS FLAIR EFFECT\\n#define R iResolution.xy\\n#define T(U) texture(iChannel0,(U)/R)\\nvec4 F (vec2 U,vec2 r) {\\n\\tvec4 t = T(U+r);\\n    return exp(-.01*dot(r,r))*(exp(2.*t)-1.);\\n}\\nvoid mainImage( out vec4 Q, vec2 U )\\n{\\n   \\n   Q = vec4(0);\\n    for (float i = 0.; i < 7.; i+=1.1) {\\n    \\tQ += F(U,+vec2(-i,i));\\n    \\tQ += F(U,+vec2(i,i));\\n    \\tQ += F(U,-vec2(-i,i));\\n    \\tQ += F(U,-vec2(i,i));\\n    }\\n    Q = T(U)*0.15+ 1e-5*Q;\\n    Q = atan(Q);\\n}\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a GLSL expert. Given a shader description, produce complete GLSL (vertex+fragment) shader code.\"}, {\"role\": \"user\", \"content\": \"Description: Earth-like planet with exaggerated features. A fbm value-noise implicit surface with shallow + deep water wave simulation, clouds, trees, mountains, desert, beaches, snow, polar caps, denoising, bloom, atmospheric scattering, and star field.\"}, {\"role\": \"assistant\", \"content\": \"// Planet implicit surface ray tracer\\n// by Morgan McGuire, @CasualEffects, http://casual-effects.com\\n//\\n// Prototype for a new Graphics Codex programming project.\\n//\\n// The key functions are the scene() distance estimator in Buf A and\\n// the renderClouds() shading in Buf B. Everything else is relatively \\n// standard ray marching infrastructure.\\n\\nmat3 planetRotation;\\n\\nconst Material ROCK = Material(Color3(0.50, 0.35, 0.15), 0.0, 0.0);\\nconst Material TREE = Material(Color3(0.05, 1.15, 0.10), 0.2, 0.1);\\nconst Material SAND = Material(Color3(1.00, 1.00, 0.85), 0.0, 0.0);\\nconst Material ICE  = Material(Color3(0.85, 1.00, 1.20), 0.2, 0.6);\\n\\n/**\\n Conservative distance estimator for the entire scene. Returns true if\\n the surface is closer than distance. Always updates distance and material.\\n The material code compiles out when called from a context that ignores it.\\n*/\\nbool scene(Point3 X, inout float distance, inout Material material, const bool shadow) { \\n    Material planetMaterial;\\n    \\n    // Move to the planet's reference frame (ideally, we'd just trace in the \\n    // planet's reference frame and avoid these operations per distance\\n    // function evaluation, but this makes it easy to integrate with a\\n    // standard framework)\\n    X = planetRotation * (X - planetCenter);\\n    Point3 surfaceLocation = normalize(X);\\n    \\n    // Compute t = distance estimator to the planet surface using a spherical height field, \\n    // in which elevation = radial distance\\n    //\\n\\t// Estimate *conservative* distance as always less than that to the bounding sphere\\n    // (i.e., push down). Work on range [0, 1], and then scale by planet radius at the end\\n    \\n\\tfloat mountain = clamp(1.0 - fbm6(surfaceLocation * 4.0) + (max(abs(surfaceLocation.y) - 0.6, 0.0)) * 0.03, 0.0, 1.0);\\n    mountain = pow3(mountain) * 0.25 + 0.8;\\n    \\n    const float water = 0.85;\\n    float elevation = mountain;    \\n    \\n    Vector3 normal = normalize(cross(dFdx(surfaceLocation * mountain), dFdy(surfaceLocation * mountain)));\\n    \\n    // Don't pay for fine details in the shadow tracing pass\\n\\tif (! shadow) {\\n        if (elevation < water) {\\n            float relativeWaterDepth = min(1.0, (water - mountain) * 30.0);\\n            const float waveMagnitude = 0.0014;\\n            const float waveLength = 0.01;\\n\\n            // Create waves. Shallow-water waves conform to coasts. Deep-water waves follow global wind patterns.\\n            const Color3 shallowWaterColor = Color3(0.4, 1.0, 1.9);\\n            // How much the waves conform to beaches\\n            const float shallowWaveRefraction = 4.0;        \\n            float shallowWavePhase = (surfaceLocation.y - mountain * shallowWaveRefraction) * (1.0 / waveLength);\\n\\n            const Color3 deepWaterColor = Color3(0, 0.1, 0.7);\\n            float deepWavePhase    = (atan(surfaceLocation.z, surfaceLocation.x) + noise(surfaceLocation * 15.0) * 0.075) * (1.5 / waveLength);\\n\\n            // This is like a lerp, but it gives a large middle region in which both wave types are active at nearly full magnitude\\n            float wave =  (cos(shallowWavePhase + time * 1.5) * sqrt(1.0 - relativeWaterDepth) + \\n                           cos(deepWavePhase + time * 2.0) * 2.5 * (1.0 - abs(surfaceLocation.y)) * square(relativeWaterDepth)) *\\n                waveMagnitude;\\n\\n            elevation = water + wave;\\n\\n            // Set material, making deep water darker\\n            planetMaterial = Material(mix(shallowWaterColor, deepWaterColor, pow(relativeWaterDepth, 0.4)), 0.5 * relativeWaterDepth, 0.7);\\n\\n            // Lighten polar water color\\n            planetMaterial.color = mix(planetMaterial.color, Color3(0.7, 1.0, 1.2), square(clamp((abs(surfaceLocation.y) - 0.65) * 3.0, 0.0, 1.0)));            \\n        } else {\\n            float materialNoise = noise(surfaceLocation * 200.0);\\n\\n            float slope = clamp(2.0 * (1.0 - dot(normal, surfaceLocation)), 0.0, 1.0);\\n\\n            bool iceCap     = abs(surfaceLocation.y) + materialNoise * 0.2 > 0.98; \\n            bool rock       = (elevation + materialNoise * 0.1 > 0.94) || (slope > 0.3);\\n            bool mountainTop = (elevation + materialNoise * 0.05 - slope * 0.05) > (planetMaxRadius * 0.92);\\n\\n            // Beach\\n            bool sand        = (elevation < water + 0.006) && (noise(surfaceLocation * 8.0) > 0.3);\\n\\n            // Equatorial desert\\n            sand = sand || (elevation < 0.89) && \\n                (noise(surfaceLocation * 1.5) * 0.15 + noise(surfaceLocation * 73.0) * 0.25 > abs(surfaceLocation.y));\\n\\n            if (rock) {\\n                // Rock\\n                planetMaterial = ROCK;\\n            } else {\\n                // Trees\\n                planetMaterial = TREE;\\n            }\\n\\n            if (iceCap || mountainTop) {\\n                // Ice (allow to slightly exceed physical conservation in the blue channel\\n                // to simulate subsurface effects)\\n                planetMaterial = ICE;\\n            } else if (! rock && sand) {\\n                planetMaterial = SAND;\\n            } else if (! rock && (iResolution.x > 420.0)) {\\n                // High frequency bumps for trees when in medium resolution\\n                elevation += noise(surfaceLocation * 150.0) * 0.02;\\n            }\\n\\n            // Add high-frequency material detail\\n            if (! sand && ! iceCap) {\\n                planetMaterial.color *= mix(noise(surfaceLocation * 256.0), 1.0, 0.4);\\n            }\\n\\n        }\\n    }\\n        \\n    elevation *= planetMaxRadius;\\n    \\n    float sampleElevation = length(X);\\n    float t = sampleElevation - elevation;\\n    \\n    // Be a little more conservative because a radial heightfield is not a great\\n    // distance estimator.\\n    t *= 0.8;\\n        \\n    // Compute output variables\\n    bool closer = (t < distance);       \\n    distance = closer ? t : distance;    \\n    if (closer) { material = planetMaterial; }\\n    return closer;    \\n}\\n\\n\\n// Version that ignores materials\\nbool scene(Point3 X, inout float distance) {\\n    Material ignoreMaterial;\\n    return scene(X, distance, ignoreMaterial, true); \\n}\\n\\nfloat distanceEstimator(Point3 X) {\\n    float d = inf;\\n    Material ignoreMaterial;\\n    scene(X, d, ignoreMaterial, false);\\n    return d;\\n}\\n\\n// Weird structure needed because WebGL does not support BREAK in a FOR loop\\nbool intersectSceneLoop(Ray R, float minDist, float maxDist, inout Surfel surfel) {\\n    const int maxIterations = 75;\\n    \\n    // Making this too large causes bad results because we use\\n    // screen-space derivatives for normal estimation.\\n    \\n    const float closeEnough = 0.0011;\\n    const float minStep = closeEnough;\\n    float closest = inf;\\n    float tForClosest = 0.0;\\n    float t = minDist;\\n    \\n    for (int i = 0; i < maxIterations; ++i) {\\n        surfel.position = R.direction * t + R.origin;\\n\\n        float dt = inf;\\n        scene(surfel.position, dt);\\n        if (dt < closest) {            \\n\\t        closest = dt;\\n            tForClosest = t;            \\n        }\\n        \\n        t += max(dt, minStep);\\n        if (dt < closeEnough) {\\n            return true;\\n        } else if (t > maxDist) {\\n            return false;\\n        }\\n    }    \\n\\n    // \\\"Screen space\\\" optimization from Mercury for shading a reasonable\\n    // point in the event of failure due to iteration count\\n    if (closest < closeEnough * 5.0) {\\n        surfel.position = R.direction * tForClosest + R.origin;\\n        return true;\\n    }\\n    \\n    return false;\\n}\\n\\n\\nbool intersectScene(Ray R, float minDist, float maxDist, inout Surfel surfel) {\\n    if (intersectSceneLoop(R, minDist, maxDist, surfel)) {\\n        const float eps = 0.0001;\\n        \\n        float d = inf;\\n        scene(surfel.position, d, surfel.material, false);\\n        surfel.normal =\\n            normalize(Vector3(distanceEstimator(surfel.position + Vector3(eps, 0, 0)), \\n                              distanceEstimator(surfel.position + Vector3(0, eps, 0)), \\n                              distanceEstimator(surfel.position + Vector3(0, 0, eps))) - \\n                              d);\\n        return true;\\n    } else {\\n        return false;\\n    }\\n}\\n\\n\\nbool shadowed(Ray R, float minDist, float maxDist) {\\n    const int maxIterations = 30;    \\n    const float closeEnough = 0.0011 * 4.0;\\n    const float minStep = closeEnough;\\n    float t = 0.0;\\n    \\n    for (int i = 0; i < maxIterations; ++i) {\\n        float dt = inf;\\n        scene(R.direction * t + R.origin, dt);        \\n        t += max(dt, minStep);\\n        if (dt < closeEnough) {\\n            return true;\\n        } else if (t > maxDist) {\\n            return false;\\n        }\\n    }\\n    \\n    return false;\\n}\\n\\n\\n\\nvoid computeReflectivities(Material material, out Color3 p_L, out Color3 p_G, out float glossyExponent) {\\n\\tp_L = mix(material.color, Color3(0.0), material.metal);\\n\\tp_G = mix(Color3(0.04), material.color, material.metal);\\n\\tglossyExponent = exp2(material.smoothness * 15.0);\\n}\\n\\n\\nRadiance3 shade(Surfel surfel, Vector3 w_i, Vector3 w_o, Biradiance3 B_i) {\\n\\tVector3 n   = surfel.normal;\\n    \\n    float cos_i = dot(n, w_i);\\n    if (cos_i < 0.0) {\\n        // Backface, don't bother shading or shadow casting\\n        return Radiance3(0.0);\\n    }\\n    \\n    // Cast a shadow ray\\n    Ray shadowRay = Ray(surfel.position + (surfel.normal + w_o) * 0.003, w_i);\\n    float shadowDist, ignore;\\n    // Find the outer bounding sphere on the atmosphere and trace shadows up to it\\n    intersectSphere(planetCenter, planetMaxRadius, shadowRay, shadowDist, ignore);\\n    if (shadowed(shadowRay, 0.0, shadowDist)) {\\n        return Radiance3(0.0);\\n    }\\n    \\n\\tColor3 p_L, p_G;\\n\\tfloat glossyExponent;\\n\\tcomputeReflectivities(surfel.material, p_L, p_G, glossyExponent);\\n\\n\\t// Compute the light contribution from the directional source\\n\\tVector3 w_h = normalize(w_i + w_o);\\n\\treturn cos_i * B_i * \\n\\t\\t// Lambertian\\n\\t\\t(p_L * (1.0 / pi) + \\n\\n\\t\\t// Glossy\\n        pow(max(0.0, dot(n, w_h)), glossyExponent) * p_G * (glossyExponent + 8.0) / (14.0 * pi));\\n}\\n\\n\\n/** Returns true if the world-space ray hits the planet */\\nbool renderPlanet(Ray eyeRay, float minDistanceToPlanet, float maxDistanceToPlanet, inout Radiance3 L_o, inout Point3 hitPoint) {    \\n    Surfel surfel;\\n    \\n    if (intersectScene(eyeRay, minDistanceToPlanet, maxDistanceToPlanet, surfel)) {\\n        // Render the planet\\n        Radiance3 L_directOut = shade(surfel, w_i, -eyeRay.direction, B_i);\\n\\n        // Clouds vary fairly slowly in elevation, so we can just measure at the\\n        // surface as an estimate of the density above the surface\\n        float cloudShadow = pow4(1.0 - clamp(cloudDensity(surfel.position, time), 0.0, 1.0));\\n        \\n        // \\\"Ambient\\\"\\n        Irradiance3 E_indirectIn = max(Irradiance3(0), Irradiance3(0.4) - 0.4 * Irradiance3(surfel.normal.yxx)); \\n        Radiance3 L_indirectOut = \\n            mix(E_indirectIn * surfel.material.color,\\n                mix(Color3(1.0), surfel.material.color, surfel.material.metal) * texture(iChannel0, reflect(w_i, surfel.normal)).rgb * 2.7, surfel.material.smoothness) * (1.0 / pi);\\n        \\n        hitPoint = surfel.position;\\n        L_o = (L_directOut + L_indirectOut) * cloudShadow;\\n\\n        if (debugMaterials) {\\n            L_o = surfel.material.color;\\n        }\\n            \\n        return true;\\n    } else {\\n        // Missed the bounding sphere or final ray-march\\n        return false;\\n    }    \\n}\\n\\n\\n\\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\\n\\t// Rotate over time\\n\\tfloat yaw   = -((iMouse.x / iResolution.x) * 2.5 - 1.25) + (autoRotate ? -time * 0.015 : 0.0);\\n\\tfloat pitch = ((iMouse.y > 0.0 ? iMouse.y : iResolution.y * 0.3) / iResolution.y) * 2.5 - 1.25;\\n \\tplanetRotation = \\n    \\tmat3(cos(yaw), 0, -sin(yaw), 0, 1, 0, sin(yaw), 0, cos(yaw)) *\\n    \\tmat3(1, 0, 0, 0, cos(pitch), sin(pitch), 0, -sin(pitch), cos(pitch));\\n\\n    \\n    Vector2 invResolution = 1.0 / iResolution.xy;\\n\\t\\n\\t// Outgoing light\\n\\tRadiance3 L_o;\\n\\t\\n\\tSurfel surfel;\\t\\n\\t\\n\\tRay eyeRay = Ray(Point3(0.0, 0.0, 5.0), normalize(Vector3(fragCoord.xy - iResolution.xy / 2.0, iResolution.y / (-2.0 * tan(verticalFieldOfView / 2.0)))));\\n\\t    \\n    Point3 hitPoint;    \\n    float minDistanceToPlanet, maxDistanceToPlanet;\\n        \\n    bool hitBounds = (showClouds || showPlanet) && intersectSphere(planetCenter, planetMaxRadius, eyeRay, minDistanceToPlanet, maxDistanceToPlanet);\\n\\n    Color3 shadowedAtmosphere = shadowedAtmosphereColor(fragCoord, iResolution, 0.5);\\n    \\n    if (hitBounds && renderPlanet(eyeRay, minDistanceToPlanet, maxDistanceToPlanet, L_o, hitPoint)) {\\n        // Tint planet with atmospheric scattering\\n        L_o = mix(L_o, shadowedAtmosphere, min(0.8, square(1.0 - (hitPoint.z - planetCenter.z) * (1.0 / planetMaxRadius))));\\n        // Update distance\\n        maxDistanceToPlanet = min(maxDistanceToPlanet, dot(eyeRay.direction, hitPoint - eyeRay.origin));\\n    } else if (showBackground) {\\n        // Background starfield\\n        float galaxyClump = (pow(noise(fragCoord.xy * (30.0 * invResolution.x)), 3.0) * 0.5 +\\n            pow(noise(100.0 + fragCoord.xy * (15.0 * invResolution.x)), 5.0)) / 1.5;\\n        L_o = Color3(galaxyClump * pow(hash(fragCoord.xy), 1500.0) * 80.0);\\n        \\n        // Color stars\\n        L_o.r *= sqrt(noise(fragCoord.xy) * 1.2);\\n        L_o.g *= sqrt(noise(fragCoord.xy * 4.0));\\n        \\n        // Twinkle\\n        L_o *= noise(time * 0.5 + fragCoord.yx * 10.0);\\n        vec2 delta = (fragCoord.xy - iResolution.xy * 0.5) * invResolution.y * 1.1;\\n        float atmosphereRadialAttenuation = min(1.0, 0.06 * pow8(max(0.0, 1.0 - (length(delta) - 0.9) / 0.9)));\\n        \\n        // Gradient around planet\\n        float radialNoise = mix(1.0, noise(normalize(delta) * 40.0 + iTime * 0.5), 0.14);\\n        L_o += radialNoise * atmosphereRadialAttenuation * shadowedAtmosphere;\\n    }   \\n        \\n\\tfragColor.xyz = L_o;\\n    fragColor.a   = maxDistanceToPlanet;\\n}\"}]}\n",
      "\n",
      "Sample from ./data/shadertoy-1000_val.jsonl:\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a GLSL expert. Given a shader description, produce complete GLSL (vertex+fragment) shader code.\"}, {\"role\": \"user\", \"content\": \"Description: Fluid-like continuous cellular automata.\"}, {\"role\": \"assistant\", \"content\": \"void mainImage( out vec4 fragColor, in vec2 fragCoord )\\n{\\n    vec2 texel = 1. / iResolution.xy;\\n    vec2 uv = fragCoord.xy / iResolution.xy;\\n    vec3 components = texture(iChannel0, uv).xyz;\\n    vec3 norm = normalize(components);\\n    fragColor = vec4(0.5 + norm.z);\\n}\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a GLSL expert. Given a shader description, produce complete GLSL (vertex+fragment) shader code.\"}, {\"role\": \"user\", \"content\": \"Description: Constructing some objects around a winding path.\"}, {\"role\": \"assistant\", \"content\": \"/*\\n\\n\\tWinding Menger Tunnel\\n\\t---------------------\\n\\n\\tI got bored and decided to wrap a Menger object around a curvy tunnel, then I got even more bored and \\n\\tincorporated some tubing and some curved screens... I have no idea what they're for either. :)\\n\\n\\tAnyway, if you put aside the cheesy, Quake-2-style graphics, it's nothing more than a couple of \\n\\tinterwoven fractal objects perturbed sinusoidally about the \\\"XY\\\" plane. In code:\\n\\n\\tpos.xy -= sinPath(pos.z);\\n\\tdist = FractalObjects(pos);\\n\\n\\tObviously, the camera has to follow the path as well, but that's basically it. You can ignore everything \\n\\telse, which is just less-than-adequate window dressing. I've been on a bit of an oldschool demo trip \\n\\tlately, which probably explains the simplistic lighting style.\\n\\n\\tOther tunnel related examples worth looking at:\\n\\n\\t// Awesome example. Makes the lighting effort in this one look lazy... which it is. :)\\n    Castle Tunnel - Hamneggs\\n    https://www.shadertoy.com/view/Xs3Xzn\\n\\n    // Love this. It inspired me to interweave the metal tubing in this particular shader.\\n    Metro Tunnel - fb39ca4\\n    https://www.shadertoy.com/view/ldsGRS\\n\\n    // Like all of dr2's stuff, it has a higher level of difficulty. :)\\n    Gotthard Tunnel - dr2\\n    https://www.shadertoy.com/view/MlSXRR \\n\\n*/\\n\\n\\n// Used to identify individual scene objects. In this case, there are only three: The metal framework, the walls,\\n// and the lights.\\nfloat objID = 0.; // Metal = 1., Walls = 2., Screens = 3..\\n\\n// Simple hash function.\\nfloat hash( float n ){ return fract(cos(n)*45758.5453); }\\n\\n\\n// Tri-Planar blending function. Based on an old Nvidia writeup:\\n// GPU Gems 3 - Ryan Geiss: http://http.developer.nvidia.com/GPUGems3/gpugems3_ch01.html\\nvec3 tex3D( sampler2D tex, in vec3 p, in vec3 n ){\\n   \\n    n = max(abs(n), 0.001); // n = max((abs(n) - 0.2)*7., 0.001); // n = max(abs(n), 0.001), etc.\\n    n /= (n.x + n.y + n.z); \\n\\treturn (texture(tex, p.yz)*n.x + texture(tex, p.zx)*n.y + texture(tex, p.xy)*n.z).xyz;\\n    \\n}\\n\\n// Common formula for rounded squares, for all intended purposes.\\nfloat lengthN(in vec2 p, in float n){ p = pow(abs(p), vec2(n)); return pow(p.x + p.y, 1.0/n); }\\n\\n// 2D path displacement.\\nvec2 path(in float x){\\n    \\n    //return vec2(0); // Trivial, straight path.\\n    //return vec2(cos(x*0.25)*1.8 + cos(x*0.15)*2., 0); // Perturbing \\\"X\\\" only.\\n    return vec2(cos(x*0.25)*1.8 + cos(x*0.15)*1.5, sin(x*0.25)*1.2 + sin(x*0.15)); // Perturbing \\\"X\\\" and \\\"Y.\\\"\\n    \\n    \\n}\\n\\n// Camera path. Arranged to coincide with the frequency of the tunnel.\\nvec3 camPath(float t){\\n  \\n    return vec3(path(t), t);\\n    \\n}\\n\\n// Smooth minimum function. There are countless articles, but IQ explains it best here:\\n// https://iquilezles.org/articles/smin\\nfloat sminP( float a, float b, float s ){\\n\\n    float h = clamp( 0.5+0.5*(b-a)/s, 0.0, 1.0 );\\n    return mix( b, a, h ) - s*h*(1.0-h);\\n}\\n\\n// I have a \\\"Menger Sponge Variation\\\" example somewhere, if you'd like to look into this.\\nfloat Menger(in vec3 q){\\n    \\n    float s = 4.;\\n    // Layer one. The \\\".05\\\" on the end varies the hole size.\\n \\tvec3 p = abs(fract(q/s)*s - s*.5);\\n \\tfloat d = min(max(p.x, p.y), min(max(p.y, p.z), max(p.x, p.z))) - s/3.;// + .05;\\n    \\n    s /= 2.;\\n    // Layer two.\\n    p = abs(fract(q/s)*s - s*.5);\\n \\td = max(d, min(max(p.x, p.y), min(max(p.y, p.z), max(p.x, p.z))) - s/3.);//+ .05\\n   \\n    s /= 3.;\\n    // Layer three. 3D space is divided by two, instead of three, to give some variance.\\n    p = abs(fract(q/s)*s - s*.5);\\n \\td = max(d, min(max(p.x, p.y), min(max(p.y, p.z), max(p.x, p.z))) - s/3.); //- .015 \\n    \\n    \\n    //float floor = max(abs(q.x) - 2., abs(q.y) - 1.5);//abs(q.x) - 2.;//\\n    //return q.y + .8;\\n    return min(d, q.y + .8);\\n \\n}\\n\\n// I have a \\\"Steel Lattice\\\" example somewhere, if you'd like to look into this. There's not\\n// much to it, though.\\nfloat tubing(in vec3 p){\\n \\n    // SECTION 1\\n    //\\n    // Repeat field entity one, which is just some tubes repeated in all directions every \\n    // two units, then combined with a smooth minimum function. Otherwise known as a lattice.\\n    p = fract(p/2.)*2. - 1.;\\n    float x1 = sminP(length(p.xy),sminP(length(p.yz),length(p.xz), 0.25), 0.25)-0.5; // EQN 1\\n    //float x1 = sqrt(min(dot(p.xy, p.xy),min(dot(p.yz, p.yz),dot(p.xz, p.xz))))-0.5; // EQN 2\\n    //p = abs(p); float x1 = min(max(p.x, p.y),min(max(p.y, p.z),max(p.x, p.z)))-0.5; // EQN 3\\n\\n    // SECTION 2\\n    //\\n    // Repeat field entity two, which is just an abstract object repeated every half unit. \\n    p = abs(fract(p*2.)*.5 - .25);\\n    //float x2 = min(p.x,min(p.y,p.z)); // EQN 1\\n    //float x2 = min(max(p.x, p.y),min(max(p.y, p.z),max(p.x, p.z)))-0.15; //-0.175, etc. // EQN 2\\n    float x2 = min(p.x, min(p.y,p.z))-.025; // EQN 1\\n    \\n    // SECTION 3\\n    //\\n    // Combining the two entities above.\\n    return max(abs(x1), abs(x2)) - .0175;\\n    \\n}\\n\\n// Creating the scene geometry. This is the process:\\n//\\n// Use the sinusoidal path function to perturb the original position. Create the Menger object\\n// using the perturbed postion. Do the same for the tubing and again with the screens.\\n// Return the minimum of the objects, and also use the relative minimums to return the object's\\n// individual ID. That's basically it.\\nfloat map(in vec3 p){\\n   \\n\\n\\t// Partial anti-warping solution, based on Gaz's \\\"Square Sin Curve\\\" shader below:\\n    // https://www.shadertoy.com/view/MscGzf\\n    //\\n    // As you could imagine, tunnels (columns) get a little warped when you bend them. Countering \\n    // that by taking the curvature into account helps quite a bit. Unfortunately, it slows things \\n    // down, so isn't being used here, which is a shame, because I like it a lot more. Anyway, if \\n    // you can spare the cycles, it gives the tunnel's \\\"X\\\" and \\\"Y\\\" (width and height) dimensions a \\n    // little more consistency.\\n    //vec2 g = (path(p.z + 0.01) - path(p.z - 0.01))/0.02;\\n    //g = cos(atan(g));\\n    \\n    \\n    // \\\"Windy Tunnels 101\\\" - Use \\\"Z\\\" to perturb the \\\"XY\\\" plane. If you're not sure how it'd done,\\n    // I have a few tunnel examples where I explain the process.\\n    p.xy -= path(p.z);\\n    \\n    //p.xy *= g; // See the anti-warping explanation above.\\n\\n\\n    // A bit of tubing, using a combination of repeat objects.\\n    float tube = tubing(p);\\n    \\n    \\n    // Again a little expensive, but it's a surprisingly effective way to bump the tunnel walls.\\n    // This is a variation, but you can thank \\\"aeikick\\\" for this little snippet. :)\\n    //vec3 u = p;\\n    //p.x -= sign(u.x)*(texture(iChannel0, u.yz/8.).x - .0)*.03;//-.2;\\n\\t//p.y -= sign(u.y)*(texture(iChannel0, u.xz/8.).x - .0)*.03;  \\n\\n    \\n    // The walls. I have another Menger example, if you'd like to look into that more closely.\\n    float walls = Menger(p);\\n    // Simpler alternatives.\\n    //float walls = 1. - max(abs(p.x), abs(p.y));\\n    //float walls = 1.25 - lengthN(p.xy, 4.0);\\n    \\n    // The curved screens. Kind of worth the effort, but not really. Fine details always overcomplicate \\n    // things, not to mention, halve the frame rate. :) Anyway, it's basically repeated square box-related \\n    // stuff... Add this, take that, etc. Fiddly, hacky, not all that interesting, and probably not the\\n    // best way to do it. Chipping away at a cylinder might raymarch better. \\n    //\\n    p += vec3(sign(p.x)*(-.11 + (sin(p.z*3.14159*2. + 1.57/1.))*.05), 0., 0.); // Screen curve, and repositioning.\\n    vec3 q = abs(mod(p + vec3(.0, .5, 0.), vec3(1., 1., 2.)) - vec3(.5, .5, 1.)); // Repeat space.\\n    float screen = max(max(q.y, q.z) - .22, q.x-.05); // Box.\\n    screen = max(screen, max(abs(p.x) - .5, abs(p.y) - .22)); // Chopping off anything outside the tunnel... Kind of.\\n    \\n    // Object ID: Equivalent to: if(tube<walls)objID=2; else objID = 1.; //etc.\\n    //\\n    // By the way, if you need to identify multiple objects, you're better off doing it in a seperate pass, \\n    // after the raymarching function. Having multiple \\\"if\\\" statements in a distance field equation can slow \\n    // things down considerably. Alternatively, there's the \\\"vec2 objA = vec2(objectADist, objAID)\\\" option \\n    // that many are fond of. It seems to be slower on my machines, but seems to work well enough.\\n    objID = 1. + step(tube, walls) + step(screen, tube)*step(screen, walls)*2.;\\n   \\n\\n    // Returning the minimum of the three objects.\\n    return min(min(tube, walls), screen);\\n    \\n/*    \\n    //Two object combinations. Spoils the illusion, but helps visualize things.\\n\\n    //objID = 2. + step(screen, tube);\\n    //return min(tube, screen); \\n        \\n        \\n    //objID = 1. + step(tube, walls);\\n    //return min(tube, walls);        \\n        \\n    objID = 1. + step(screen, walls)*2.;\\n    return min(screen, walls);    \\n*/     \\n    \\n    \\n}\\n\\n\\n\\n// Tetrahedral normal, to save a couple of \\\"map\\\" calls. Courtesy of IQ.\\nvec3 calcNormal(in vec3 p){\\n\\n    // Note the slightly increased sampling distance, to alleviate artifacts due to hit point inaccuracies.\\n    vec2 e = vec2(0.0025, -0.0025); \\n    return normalize(e.xyy * map(p + e.xyy) + e.yyx * map(p + e.yyx) + e.yxy * map(p + e.yxy) + e.xxx * map(p + e.xxx));\\n}\\n\\n/*\\n// Standard normal function.\\nvec3 calcNormal(in vec3 p) {\\n\\tconst vec2 e = vec2(0.005, 0);\\n\\treturn normalize(vec3(map(p + e.xyy) - map(p - e.xyy), map(p + e.yxy) - map(p - e.yxy),\\tmap(p + e.yyx) - map(p - e.yyx)));\\n}\\n*/\\n\\n// I keep a collection of occlusion routines... OK, that sounded really nerdy. :)\\n// Anyway, I like this one. I'm assuming it's based on IQ's original .\\nfloat calcAO(in vec3 pos, in vec3 nor)\\n{\\n\\tfloat sca = 2.0, occ = 0.0;\\n    for( int i=0; i<5; i++ ){\\n    \\n        float hr = 0.01 + float(i)*0.5/4.0;        \\n        float dd = map(nor * hr + pos);\\n        occ += (hr - dd)*sca;\\n        sca *= 0.7;\\n    }\\n    return clamp( 1.0 - occ, 0.0, 1.0 );    \\n}\\n\\n\\n// Texture bump mapping. Four tri-planar lookups, or 12 texture lookups in total. I tried to \\n// make it as concise as possible. Whether that translate to speed, or not, I couldn't say.\\nvec3 texBump( sampler2D tx, in vec3 p, in vec3 n, float bf){\\n   \\n    const vec2 e = vec2(0.002, 0);\\n    \\n    // Three gradient vectors rolled into a matrix, constructed with offset greyscale texture values.    \\n    mat3 m = mat3( tex3D(tx, p - e.xyy, n), tex3D(tx, p - e.yxy, n), tex3D(tx, p - e.yyx, n));\\n    \\n    vec3 g = vec3(0.299, 0.587, 0.114)*m; // Converting to greyscale.\\n    g = (g - dot(tex3D(tx,  p , n), vec3(0.299, 0.587, 0.114)) )/e.x; g -= n*dot(n, g);\\n                      \\n    return normalize( n + g*bf ); // Bumped normal. \\\"bf\\\" - bump factor.\\n\\t\\n}\\n\\n// Standard hue rotation formula.\\nvec3 rotHue(vec3 p, float a){\\n\\n    vec2 cs = sin(vec2(1.570796, 0) + a);\\n\\n    mat3 hr = mat3(0.299,  0.587,  0.114,  0.299,  0.587,  0.114,  0.299,  0.587,  0.114) +\\n        \\t  mat3(0.701, -0.587, -0.114, -0.299,  0.413, -0.114, -0.300, -0.588,  0.886) * cs.x +\\n        \\t  mat3(0.168,  0.330, -0.497, -0.328,  0.035,  0.292,  1.250, -1.050, -0.203) * cs.y;\\n\\t\\t\\t\\t\\t\\t\\t \\n    return clamp(p*hr, 0., 1.);\\n}\\n\\n// Screen pattern. Simple, but effective. The idea to go with this was inspired by Dmitry Andreev's \\n// really cool \\\"pixelScreen\\\" shader, here: https://www.shadertoy.com/view/XdG3Wc\\n//\\n// His example is a little fancier, mainly because he's using way more code... The fact that he won \\n// Assembly a couple of times might also be a factor. :)\\nfloat dotPattern(vec2 p){\\n    \\n    // Partition space into multiple squares.\\n    vec2 fp = abs(fract(p)-0.5)*2.;\\n    \\n    // Rounded circle, for the overlay, or vignette, if you prefer.\\n    fp = pow(fp, vec2(8.));\\n    float r = max(1. - pow(fp.x + fp.y, 1.), 0.);\\n    \\n    // More squarish (Chebyshev) version of the above.\\n    //fp = pow(fp, vec2(8.));\\n    //float r = 1. - max(fp.x, fp.y);\\n    \\n    // Single value for each square. Used for IDs and a bunch of other things, but in this \\n    // case it'll give the square a homogeneous color.\\n    p = floor(p); \\n    \\n    // The blocky pattern value. Made up, but you could use all kinds of things, like Voronoi, etc. \\n    float c = dot(sin(p/4. - cos(p.yx/.2 + iTime/4.)), vec2(.5));\\n\\n    c = fract(c * 7.0); // Mixing it up, for no particular reason.\\n\\n    return c*r; // Pixel shade, multiplied by the rounded square vignette. Range: [0, 1].\\n    \\n}\\n\\n\\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord ){\\n   \\n    \\n\\t// Screen coordinates.\\n\\tvec2 u = (fragCoord - iResolution.xy*0.5)/iResolution.y;\\n\\t\\n\\t// Camera Setup.\\n    vec3 ro = camPath(iTime*1.5); // Camera position, doubling as the ray origin.\\n    vec3 lk = camPath(iTime*1.5 + .1);  // \\\"Look At\\\" position.\\n    vec3 lp = camPath(iTime*1.5 + 2.) + vec3(0, 2, 0); // Light position, somewhere near the moving camera.\\n\\n\\n    // Using the above to produce the unit ray-direction vector.\\n    float FOV = 1.57; // FOV - Field of view.\\n    vec3 fwd = normalize(lk-ro);\\n    vec3 rgt = normalize(vec3(fwd.z, 0., -fwd.x )); \\n    vec3 up = cross(fwd, rgt);\\n\\n    // Unit direction ray.\\n    vec3 rd = normalize(fwd + FOV*(u.x*rgt + u.y*up));\\n \\n    vec2 a = sin(vec2(1.5707963, 0) - camPath(lk.z).x/12.); \\n    mat2 rM = mat2(a, -a.y, a.x);\\n    rd.xy = rd.xy*rM; // Apparently, \\\"rd.xy *= rM\\\" doesn't work on some setups. Crazy.\\n    \\n    // Mouse controls, as per Dave Hoskins's suggestion. A bit hacky, but I'll fix them.    \\n\\tvec2 ms = vec2(0);\\n    if (iMouse.z > 1.0) ms = (2.*iMouse.xy - iResolution.xy)/iResolution.xy;\\n    a = sin(vec2(1.5707963, 0) - ms.x); \\n    rM = mat2(a, -a.y, a.x);\\n    rd.xz = rd.xz*rM; \\n    a = sin(vec2(1.5707963, 0) - ms.y); \\n    rM = mat2(a, -a.y, a.x);\\n    rd.yz = rd.yz*rM;\\n    \\n\\n    \\n    // Raymarching.\\n    const float FAR = 50.0;\\n    float t = 0.0, h;\\n    for(int i = 0; i < 96; i++){\\n    \\n        h = map(ro+rd*t);\\n        // Note the \\\"t*b + a\\\" addition. Basically, we're putting less emphasis on accuracy, as\\n        // \\\"t\\\" increases. It's a cheap trick that works in most situations... Not all, though.\\n        if(abs(h)<0.001*(t*.75 + .25) || t>FAR) break;//*(t*.5 + 1.)\\n        t += h*.75;\\n        //t += step(.5, t)*h*.25 + h*.5;\\n        \\n    }\\n    \\n    // Initialize the scene color.\\n    vec3 col = vec3(0);\\n    \\n    // Scene hit, so color the pixel. \\n    if(t<FAR){\\n        \\n        // This looks a little messy and haphazard, but it's really just some basic lighting, and application\\n        // of the following material properties: Metal = 1., Walls = 2., Screens = 3..\\n    \\n        float ts = 2.;\\n        // Global object ID. It needs to be saved just after the raymarching equation, since other \\\"map\\\" calls,\\n        // like normal calculations will give incorrect results. Found that out the hard way. :)\\n        float saveObjID = objID; \\n        \\n        \\n        vec3 pos = ro + rd*t; // Scene postion.\\n        vec3 pOffs = pos - vec3(camPath(pos.z).xy, 0); // Postion, offset by the path. \\n        vec3 nor = calcNormal(pos); // Normal.\\n        \\n        // Apply some subtle texture bump mapping to the walls and the metal tubing, but not the screen.\\n        // I should probably get rid of that \\\"if\\\" statement later, but it seems OK for now.\\n        if(saveObjID<2.5) nor = texBump(iChannel0, pOffs*ts, nor, 0.002 + step(saveObjID, 1.5)*0.012);\\n        \\n        \\n\\t\\tcol = tex3D(iChannel0, pOffs*ts, nor); // Texture pixel at the scene postion.\\n        col = smoothstep(-.3, .8, col)*vec3(1., .8, .7); // Process the color a little.\\n\\n        // More fake lighting. This was just a bit of trial-and-error to produce some repetitive,\\n        // slightly overhead, spotlights in each of the modules. Cylinder in XY, sine repeat\\n        // in the Z direction... Something like that.\\n        float spot = max(2. - length(pOffs.xy - vec2(0, 1)), 0.)*(sin((pOffs.z)*3.14159 + 1.57)*.5+.5);\\n        spot = smoothstep(0.25, 1., spot);\\n        \\n        \\n        \\n        float occ = calcAO( pos, nor ); // Occlusion.\\n\\t\\tvec3  li = normalize( lp - pos ); // Point light.\\n        float dif = clamp(dot(nor, li), 0.0, 1.0); // Diffuse.\\n        float spe = pow(max(dot(reflect(-li, nor), -rd), 0.), 8.); // Object specular.\\n        float spe2 = 0.; // Global specular.\\n\\n            \\n\\n        vec3  rCol = vec3(0); // Reflection color. Mostly fake.\\n        \\n        // If the metal tubing or the screen is hit, apply the individual properties.\\n        if(saveObjID>1.5){ \\n\\t\\t\\t\\n            // Grey out the limestone wall color.\\n            col = vec3(1)*dot(col*.7+.2, vec3(.299, .587, .114));\\n            // Add some fake reflection. Not reliable, but it's subtle.\\n            rCol = tex3D(iChannel0, (pOffs + reflect(rd, nor))*ts, nor);\\n            col += rCol*.25 + spot*.125;\\n            spe2 = spe*spe*.25; // Ramp up the global specular a bit.\\n            if(saveObjID<2.5) dif = pow(dif, 2.)*1.25;\\n            \\n        }         \\n        \\n        // If just the screen has been hit, apply some extra properties, then draw the screen image.\\n        // I could just write \\\"saveObjID == 3.,\\\" but I get a little paranoid where floats are concerned. :)\\n        if(saveObjID>2.5){\\n\\n            // For the screen image, we're interested in the offset height and depth positions. Ie: pOffs.zy.\\n            \\n            // Pixelized dot pattern shade.\\n            float c = dotPattern(pOffs.zy*36.+.5);\\n            \\n            // Applying some color to the shade.\\n            col = vec3(min(c*1.5, 1.), pow(c, 2.5), pow(c, 12.));\\n            // Mixing the colors around a little. Made up.\\n            col = mix(col.zyx, col, sin(dot(pos, vec3(.333))*3.14159*6.)*.34+.66);\\n\\t\\t\\t\\n            // Individual screen ID or sorts.\\n            float id = hash(dot(floor(pOffs + vec3(.0, .5, .5)), vec3(7, 157, 113)));\\n            \\n            // Use the screen ID to give it a different random hue.\\n            col = rotHue(col, floor(id*12.)/12.*6.283/2.); \\n            \\n            col += rCol*rCol*.5; // Screen reflection.            \\n            \\n            dif += .5; // Make the screen appear self illuminating, but increasing the diffuse.\\n            spe += .25;\\n            \\n        }\\n       \\n        // Combining everything together to produce the scene color.\\n        col *= (dif + .25 + spot*.5 + vec3(.25, .3, .5)*spe) + spe2;\\n        col *= occ; // Applying occlusion.\\n       \\n        \\n    }\\n    \\n    // Applying some very slight fog in the distance. This is technically an inside scene...\\n    // Or is it underground... Who cares, it's just a shader. :)\\n    col = mix(min(col, 1.), vec3(0), 1.-exp(-t*t/FAR/FAR*15.));//smoothstep(0., FAR-20., t)\\n    \\n    // Done.\\n    fragColor = vec4(col, 1.0);\\n    \\n}\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a GLSL expert. Given a shader description, produce complete GLSL (vertex+fragment) shader code.\"}, {\"role\": \"user\", \"content\": \"Description: the idea is to rotate the fractal based on distance from the camera.\\ni used it to make this gif https://media.giphy.com/media/l2QE1mlTRVy7Y7Hfa/giphy.gif\"}, {\"role\": \"assistant\", \"content\": \"#define MAXDIST 50.\\n\\nstruct Ray {\\n\\tvec3 ro;\\n    vec3 rd;\\n};\\n  \\n// from netgrind\\nvec3 hue(vec3 color, float shift) {\\n\\n    const vec3  kRGBToYPrime = vec3 (0.299, 0.587, 0.114);\\n    const vec3  kRGBToI     = vec3 (0.596, -0.275, -0.321);\\n    const vec3  kRGBToQ     = vec3 (0.212, -0.523, 0.311);\\n\\n    const vec3  kYIQToR   = vec3 (1.0, 0.956, 0.621);\\n    const vec3  kYIQToG   = vec3 (1.0, -0.272, -0.647);\\n    const vec3  kYIQToB   = vec3 (1.0, -1.107, 1.704);\\n\\n    // Convert to YIQ\\n    float   YPrime  = dot (color, kRGBToYPrime);\\n    float   I      = dot (color, kRGBToI);\\n    float   Q      = dot (color, kRGBToQ);\\n\\n    // Calculate the hue and chroma\\n    float   hue     = atan (Q, I);\\n    float   chroma  = sqrt (I * I + Q * Q);\\n\\n    // Make the user's adjustments\\n    hue += shift;\\n\\n    // Convert back to YIQ\\n    Q = chroma * sin (hue);\\n    I = chroma * cos (hue);\\n\\n    // Convert back to RGB\\n    vec3    yIQ   = vec3 (YPrime, I, Q);\\n    color.r = dot (yIQ, kYIQToR);\\n    color.g = dot (yIQ, kYIQToG);\\n    color.b = dot (yIQ, kYIQToB);\\n\\n    return color;\\n}\\n\\n// ------\\n\\n// by iq\\n\\nfloat opU( float d1, float d2 )\\n{\\n    return min(d1,d2);\\n}\\n\\nfloat smin( float a, float b, float k ){\\n    float h = clamp( 0.5+0.5*(b-a)/k, 0.0, 1.0 );\\n    return mix( b, a, h ) - k*h*(1.0-h);\\n}\\n\\nfloat length6( vec3 p )\\n{\\n\\tp = p*p*p; p = p*p;\\n\\treturn pow( p.x + p.y + p.z, 1.0/6.0 );\\n}\\n\\n// ------\\n\\n// from hg_sdf \\n\\nfloat fPlane(vec3 p, vec3 n, float distanceFromOrigin) {\\n\\treturn dot(p, n) + distanceFromOrigin;\\n}\\n\\nvoid pR(inout vec2 p, float a) {\\n\\tp = cos(a)*p + sin(a)*vec2(p.y, -p.x);\\n}\\n\\n// -------\\n\\n\\nfloat fractal(vec3 p)\\n{\\n    const int iterations = 20;\\n\\t\\n    float d = iTime*5. - p.z;\\n   \\tp=p.yxz;\\n    pR(p.yz, 1.570795);\\n    p.x += 6.5;\\n\\n    p.yz = mod(abs(p.yz)-.0, 20.) - 10.;\\n    float scale = 1.25;\\n    \\n    p.xy /= (1.+d*d*0.0005);\\n    \\n\\tfloat l = 0.;\\n\\t\\n    for (int i=0; i<iterations; i++) {\\n\\t\\tp.xy = abs(p.xy);\\n\\t\\tp = p*scale + vec3(-3. + d*0.0095,-1.5,-.5);\\n        \\n\\t\\tpR(p.xy,0.35-d*0.015);\\n\\t\\tpR(p.yz,0.5+d*0.02);\\n\\t\\t\\n        l =length6(p);\\n\\t}\\n\\treturn l*pow(scale, -float(iterations))-.15;\\n}\\n\\nvec2 map(vec3 pos) \\n{\\n    float dist = 10.; \\n    dist = opU(dist, fractal(pos));\\n    dist = smin(dist, fPlane(pos,vec3(0.0,1.0,0.0),10.), 4.6);\\n    return vec2(dist, 0.);\\n}\\n\\nvec3 vmarch(Ray ray, float dist)\\n{   \\n    vec3 p = ray.ro;\\n    vec2 r = vec2(0.);\\n    vec3 sum = vec3(0);\\n    vec3 c = hue(vec3(0.,0.,1.),5.5);\\n    for( int i=0; i<20; i++ )\\n    {\\n        r = map(p);\\n        if (r.x > .01) break;\\n        p += ray.rd*.015;\\n        vec3 col = c;\\n        col.rgb *= smoothstep(.0,0.15,-r.x);\\n        sum += abs(col)*.5;\\n    }\\n    return sum;\\n}\\n\\nvec2 march(Ray ray) \\n{\\n    const int steps = 50;\\n    const float prec = 0.001;\\n    vec2 res = vec2(0.);\\n    \\n    for (int i = 0; i < steps; i++) \\n    {        \\n        vec2 s = map(ray.ro + ray.rd * res.x);\\n        \\n        if (res.x > MAXDIST || s.x < prec) \\n        {\\n        \\tbreak;    \\n        }\\n        \\n        res.x += s.x;\\n        res.y = s.y;\\n        \\n    }\\n   \\n    return res;\\n}\\n\\nvec3 calcNormal(vec3 pos) \\n{\\n\\tconst vec3 eps = vec3(0.005, 0.0, 0.0);\\n                          \\n    return normalize(\\n        vec3(map(pos + eps).x - map(pos - eps).x,\\n             map(pos + eps.yxz).x - map(pos - eps.yxz).x,\\n             map(pos + eps.yzx).x - map(pos - eps.yzx).x ) \\n    );\\n}\\n\\nvec4 render(Ray ray) \\n{\\n    vec3 col = vec3(0.);\\n\\tvec2 res = march(ray);\\n   \\n    if (res.x > MAXDIST) \\n    {\\n        return vec4(col, 50.);\\n    }\\n    \\n    vec3 pos = ray.ro+res.x*ray.rd;\\n    ray.ro = pos;\\n   \\tcol = vmarch(ray, res.x);\\n    \\n    col = mix(col, vec3(0.), clamp(res.x/50., 0., 1.));\\n   \\treturn vec4(col, res.x);\\n}\\n\\nmat3 camera(in vec3 ro, in vec3 rd, float rot) \\n{\\n\\tvec3 forward = normalize(rd - ro);\\n    vec3 worldUp = vec3(sin(rot), cos(rot), 0.0);\\n    vec3 x = normalize(cross(forward, worldUp));\\n    vec3 y = normalize(cross(x, forward));\\n    return mat3(x, y, forward);\\n}\\n\\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\\n{\\n\\tvec2 uv = fragCoord.xy / iResolution.xy;\\n    uv = uv * 2.0 - 1.0;\\n    uv.x *= iResolution.x / iResolution.y;\\n    uv.y -= uv.x*uv.x*0.15;\\n    vec3 camPos = vec3(3., -1.5, iTime*5.);\\n    vec3 camDir = camPos+vec3(-1.25,0.1, 1.);\\n    mat3 cam = camera(camPos, camDir, 0.);\\n    vec3 rayDir = cam * normalize( vec3(uv, .8));\\n    \\n    Ray ray;\\n    ray.ro = camPos;\\n    ray.rd = rayDir;\\n    \\n    vec4 col = render(ray);\\n    \\n\\tfragColor = vec4(1.-col.xyz,clamp(1.-col.w/MAXDIST, 0., 1.));\\n}\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a GLSL expert. Given a shader description, produce complete GLSL (vertex+fragment) shader code.\"}, {\"role\": \"user\", \"content\": \"Description: Water vapour evaporates from the ocean, is carried by wind currents, then rains onto the land, painting it with vegetation.\\n\\nSee also: [url]https://www.shadertoy.com/view/MdGBWG[/url]\"}, {\"role\": \"assistant\", \"content\": \"// 2018 David A Roberts <https://davidar.io>\\n\\n// atmospheric water vapour and soil moisture\\n\\nvec4 map(vec2 uv) {\\n    vec2 p = uv * MAPRES + 0.5;\\n    vec2 i = floor(p);\\n    vec2 f = p - i;\\n    f = f*f*f*(f*(f*6.0-15.0)+10.0); // https://iquilezles.org/articles/texture\\n    p = i + f;\\n    return texture(iChannel2, (p - 0.5)/iResolution.xy);\\n}\\n\\nvec2 getVelocity(vec2 uv) {\\n    vec2 p = uv * MAPRES;\\n    if (p.x < 1.) p.x = 1.;\\n    vec2 v = texture(iChannel1, p/iResolution.xy + PASS4).xy;\\n    if (length(v) > 1.) v = normalize(v);\\n    return v;\\n}\\n\\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord ) {\\n    fragColor = vec4(0);\\n    if (iFrame < 10) return;\\n    vec2 uv = fragCoord/iResolution.xy;\\n    \\n    vec2 v = getVelocity(uv);\\n    vec2 vn = getVelocity(uv + N/iResolution.xy);\\n    vec2 ve = getVelocity(uv + E/iResolution.xy);\\n    vec2 vs = getVelocity(uv + S/iResolution.xy);\\n    vec2 vw = getVelocity(uv + W/iResolution.xy);\\n    float div = (ve - vw).x/2. + (vn - vs).y/2.;\\n    \\n    float land = map(uv).x;\\n    float height = map(uv).y;\\n    float hn = map(uv + N/iResolution.xy).y;\\n    float he = map(uv + E/iResolution.xy).y;\\n    float hs = map(uv + S/iResolution.xy).y;\\n    float hw = map(uv + W/iResolution.xy).y;\\n    vec2 hgrad = vec2(he - hw, hn - hs)/2.;\\n    \\n    vec4 climate = texture(iChannel1, uv * MAPRES / iResolution.xy + PASS3);\\n    float mbar = climate.x;\\n    float temp = climate.y;\\n    \\n    // water vapour advection\\n    float w = texture(iChannel0, fract((fragCoord - v) / iResolution.xy)).w;\\n    if (land < 0.5) { // evaporation\\n        float noise = clamp(3. * FBM(vec3(5. * fragCoord/iResolution.xy, iTime)) - 1., 0., 1.);\\n        w += noise * clamp(temp + 2., 0., 100.)/32. * (0.075 - 3. * div - 0.0045 * (mbar - 1012.));\\n    }\\n    w -= 0.01 * w; // precipitation\\n    w -= 0.3 * length(hgrad); // orographic lift\\n    fragColor.w = clamp(w, 0., 3.);\\n    \\n    // soil moisture\\n    fragColor.z = texture(iChannel0, fragCoord/iResolution.xy).z;\\n    fragColor.z = clamp(0.9985 * fragColor.z + 3. * clamp(w, 0., 0.01), 0., 5.);\\n}\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a GLSL expert. Given a shader description, produce complete GLSL (vertex+fragment) shader code.\"}, {\"role\": \"user\", \"content\": \"Description: Inspired by trisomie21's 3d noise fireball, I made some tweaks and ended up with a star.\\n\\nEDIT: Fixed the negative sqrt issue as per iq's suggestion.\"}, {\"role\": \"assistant\", \"content\": \"// based on https://www.shadertoy.com/view/lsf3RH by\\r\\n// trisomie21 (THANKS!)\\r\\n// My apologies for the ugly code.\\r\\n\\r\\nfloat snoise(vec3 uv, float res)\\t// by trisomie21\\r\\n{\\r\\n\\tconst vec3 s = vec3(1e0, 1e2, 1e4);\\r\\n\\t\\r\\n\\tuv *= res;\\r\\n\\t\\r\\n\\tvec3 uv0 = floor(mod(uv, res))*s;\\r\\n\\tvec3 uv1 = floor(mod(uv+vec3(1.), res))*s;\\r\\n\\t\\r\\n\\tvec3 f = fract(uv); f = f*f*(3.0-2.0*f);\\r\\n\\t\\r\\n\\tvec4 v = vec4(uv0.x+uv0.y+uv0.z, uv1.x+uv0.y+uv0.z,\\r\\n\\t\\t      \\t  uv0.x+uv1.y+uv0.z, uv1.x+uv1.y+uv0.z);\\r\\n\\t\\r\\n\\tvec4 r = fract(sin(v*1e-3)*1e5);\\r\\n\\tfloat r0 = mix(mix(r.x, r.y, f.x), mix(r.z, r.w, f.x), f.y);\\r\\n\\t\\r\\n\\tr = fract(sin((v + uv1.z - uv0.z)*1e-3)*1e5);\\r\\n\\tfloat r1 = mix(mix(r.x, r.y, f.x), mix(r.z, r.w, f.x), f.y);\\r\\n\\t\\r\\n\\treturn mix(r0, r1, f.z)*2.-1.;\\r\\n}\\r\\n\\r\\nfloat freqs[4];\\r\\n\\r\\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\\r\\n{\\r\\n\\tfreqs[0] = texture( iChannel1, vec2( 0.01, 0.25 ) ).x;\\r\\n\\tfreqs[1] = texture( iChannel1, vec2( 0.07, 0.25 ) ).x;\\r\\n\\tfreqs[2] = texture( iChannel1, vec2( 0.15, 0.25 ) ).x;\\r\\n\\tfreqs[3] = texture( iChannel1, vec2( 0.30, 0.25 ) ).x;\\r\\n\\r\\n\\tfloat brightness\\t= freqs[1] * 0.25 + freqs[2] * 0.25;\\r\\n\\tfloat radius\\t\\t= 0.24 + brightness * 0.2;\\r\\n\\tfloat invRadius \\t= 1.0/radius;\\r\\n\\t\\r\\n\\tvec3 orange\\t\\t\\t= vec3( 0.8, 0.65, 0.3 );\\r\\n\\tvec3 orangeRed\\t\\t= vec3( 0.8, 0.35, 0.1 );\\r\\n\\tfloat time\\t\\t= iTime * 0.1;\\r\\n\\tfloat aspect\\t= iResolution.x/iResolution.y;\\r\\n\\tvec2 uv\\t\\t\\t= fragCoord.xy / iResolution.xy;\\r\\n\\tvec2 p \\t\\t\\t= -0.5 + uv;\\r\\n\\tp.x *= aspect;\\r\\n\\r\\n\\tfloat fade\\t\\t= pow( length( 2.0 * p ), 0.5 );\\r\\n\\tfloat fVal1\\t\\t= 1.0 - fade;\\r\\n\\tfloat fVal2\\t\\t= 1.0 - fade;\\r\\n\\t\\r\\n\\tfloat angle\\t\\t= atan( p.x, p.y )/6.2832;\\r\\n\\tfloat dist\\t\\t= length(p);\\r\\n\\tvec3 coord\\t\\t= vec3( angle, dist, time * 0.1 );\\r\\n\\t\\r\\n\\tfloat newTime1\\t= abs( snoise( coord + vec3( 0.0, -time * ( 0.35 + brightness * 0.001 ), time * 0.015 ), 15.0 ) );\\r\\n\\tfloat newTime2\\t= abs( snoise( coord + vec3( 0.0, -time * ( 0.15 + brightness * 0.001 ), time * 0.015 ), 45.0 ) );\\t\\r\\n\\tfor( int i=1; i<=7; i++ ){\\r\\n\\t\\tfloat power = pow( 2.0, float(i + 1) );\\r\\n\\t\\tfVal1 += ( 0.5 / power ) * snoise( coord + vec3( 0.0, -time, time * 0.2 ), ( power * ( 10.0 ) * ( newTime1 + 1.0 ) ) );\\r\\n\\t\\tfVal2 += ( 0.5 / power ) * snoise( coord + vec3( 0.0, -time, time * 0.2 ), ( power * ( 25.0 ) * ( newTime2 + 1.0 ) ) );\\r\\n\\t}\\r\\n\\t\\r\\n\\tfloat corona\\t\\t= pow( fVal1 * max( 1.1 - fade, 0.0 ), 2.0 ) * 50.0;\\r\\n\\tcorona\\t\\t\\t\\t+= pow( fVal2 * max( 1.1 - fade, 0.0 ), 2.0 ) * 50.0;\\r\\n\\tcorona\\t\\t\\t\\t*= 1.2 - newTime1;\\r\\n\\tvec3 sphereNormal \\t= vec3( 0.0, 0.0, 1.0 );\\r\\n\\tvec3 dir \\t\\t\\t= vec3( 0.0 );\\r\\n\\tvec3 center\\t\\t\\t= vec3( 0.5, 0.5, 1.0 );\\r\\n\\tvec3 starSphere\\t\\t= vec3( 0.0 );\\r\\n\\t\\r\\n\\tvec2 sp = -1.0 + 2.0 * uv;\\r\\n\\tsp.x *= aspect;\\r\\n\\tsp *= ( 2.0 - brightness );\\r\\n  \\tfloat r = dot(sp,sp);\\r\\n\\tfloat f = (1.0-sqrt(abs(1.0-r)))/(r) + brightness * 0.5;\\r\\n\\tif( dist < radius ){\\r\\n\\t\\tcorona\\t\\t\\t*= pow( dist * invRadius, 24.0 );\\r\\n  \\t\\tvec2 newUv;\\r\\n \\t\\tnewUv.x = sp.x*f;\\r\\n  \\t\\tnewUv.y = sp.y*f;\\r\\n\\t\\tnewUv += vec2( time, 0.0 );\\r\\n\\t\\t\\r\\n\\t\\tvec3 texSample \\t= texture( iChannel0, newUv ).rgb;\\r\\n\\t\\tfloat uOff\\t\\t= ( texSample.g * brightness * 4.5 + time );\\r\\n\\t\\tvec2 starUV\\t\\t= newUv + vec2( uOff, 0.0 );\\r\\n\\t\\tstarSphere\\t\\t= texture( iChannel0, starUV ).rgb;\\r\\n\\t}\\r\\n\\t\\r\\n\\tfloat starGlow\\t= min( max( 1.0 - dist * ( 1.0 - brightness ), 0.0 ), 1.0 );\\r\\n\\t//fragColor.rgb\\t= vec3( r );\\r\\n\\tfragColor.rgb\\t= vec3( f * ( 0.75 + brightness * 0.3 ) * orange ) + starSphere + corona * orange + starGlow * orangeRed;\\r\\n\\tfragColor.a\\t\\t= 1.0;\\r\\n}\"}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- DEBUG: Inspect your DataFrame ---\n",
    "print(\"Columns in DataFrame:\", df.columns.tolist())\n",
    "print(\"First row of DataFrame:\\n\", df.iloc[0])\n",
    "\n",
    "# --- Conversion Functions ---\n",
    "def create_jsonl_dataset(df, output_path, system_prompt, validation_split=0.1, max_samples=None):\n",
    "    \"\"\"\n",
    "    Convert DataFrame to JSONL format for OpenAI fine-tuning\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas DataFrame with shader data\n",
    "        output_path: Path to save the JSONL file\n",
    "        system_prompt: System prompt to use for all examples\n",
    "        validation_split: Fraction of data to use for validation\n",
    "        max_samples: Maximum number of samples to include (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_path, val_path)\n",
    "    \"\"\"\n",
    "    # Make a copy and shuffle\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    if max_samples and max_samples < len(df_copy):\n",
    "        df_copy = df_copy.iloc[:max_samples]\n",
    "    \n",
    "    # Calculate split indices\n",
    "    val_size = int(len(df_copy) * validation_split)\n",
    "    train_df = df_copy.iloc[val_size:]\n",
    "    val_df = df_copy.iloc[:val_size]\n",
    "    \n",
    "    train_path = f\"{output_path}_train.jsonl\"\n",
    "    val_path = f\"{output_path}_val.jsonl\"\n",
    "    \n",
    "    # Process training data\n",
    "    print(f\"Creating training set with {len(train_df)} examples...\")\n",
    "    _create_jsonl_file(train_df, train_path, system_prompt)\n",
    "    \n",
    "    # Process validation data\n",
    "    print(f\"Creating validation set with {len(val_df)} examples...\")\n",
    "    _create_jsonl_file(val_df, val_path, system_prompt)\n",
    "    \n",
    "    return train_path, val_path\n",
    "\n",
    "def _safe_strip(val):\n",
    "    \"\"\"Convert to string and strip, or return empty string if missing/NaN.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    return str(val).strip()\n",
    "\n",
    "def _create_jsonl_file(df, output_path, system_prompt):\n",
    "    \"\"\"Helper function to create a single JSONL file\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path) or \".\", exist_ok=True)\n",
    "    n_written = 0\n",
    "    n_skipped = 0\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # Try the most likely column names, fallback to empty string\n",
    "        desc = _safe_strip(row.get('description', ''))\n",
    "        code = _safe_strip(row.get('code', ''))\n",
    "        \n",
    "        # Skip if code is empty\n",
    "        if not code:\n",
    "            n_skipped += 1\n",
    "            if n_skipped <= 5:\n",
    "                print(f\"Skipping row {idx} (empty code): desc='{desc}'\")\n",
    "            continue\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Description: {desc}\"},\n",
    "            {\"role\": \"assistant\", \"content\": code}\n",
    "        ]\n",
    "        with open(output_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(json.dumps({\"messages\": messages}) + \"\\n\")\n",
    "        n_written += 1\n",
    "    print(f\"Written: {n_written}, Skipped: {n_skipped}\")\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "system_prompt = \"You are a GLSL expert. Given a shader description, produce complete GLSL (vertex+fragment) shader code.\"\n",
    "\n",
    "train_path, val_path = create_jsonl_dataset(\n",
    "    df, \n",
    "    './data/shadertoy-1000',\n",
    "    system_prompt,\n",
    "    validation_split=0.1,\n",
    "    max_samples=None  # Set a number like 100 for testing, or None for all\n",
    ")\n",
    "\n",
    "print(f\"Training data saved to: {train_path}\")\n",
    "print(f\"Validation data saved to: {val_path}\")\n",
    "\n",
    "# --- Show a sample of the output ---\n",
    "def print_jsonl_sample(path, n=5):\n",
    "    print(f\"\\nSample from {path}:\")\n",
    "    if not os.path.exists(path):\n",
    "        print(\"File does not exist.\")\n",
    "        return\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(line.strip())\n",
    "            if i + 1 >= n:\n",
    "                break\n",
    "\n",
    "print_jsonl_sample(train_path)\n",
    "print_jsonl_sample(val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42542d9",
   "metadata": {},
   "source": [
    "## 4. Validate the JSONL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "130eed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data statistics:\n",
      " - examples: 1370\n",
      " - input_tokens: 86125\n",
      " - output_tokens: 2630566\n",
      " - total_tokens: 2716691\n",
      " - estimated_cost: 13.583454999999999\n",
      "\n",
      "Validation data statistics:\n",
      " - examples: 152\n",
      " - input_tokens: 9382\n",
      " - output_tokens: 285599\n",
      " - total_tokens: 294981\n",
      " - estimated_cost: 1.474905\n",
      "\n",
      "Estimated total cost for 2 epochs: $28.64\n"
     ]
    }
   ],
   "source": [
    "# Check basic stats of prepared files\n",
    "import json\n",
    "\n",
    "def count_tokens_in_jsonl(file_path):\n",
    "    \"\"\"Count tokens in a JSONL file and calculate expected costs\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    examples = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        example = json.loads(line)\n",
    "        messages = example['messages']\n",
    "        \n",
    "        # Rough token estimation (4 chars ≈ 1 token)\n",
    "        for msg in messages:\n",
    "            if msg['role'] in ['system', 'user']:\n",
    "                input_tokens += len(msg['content']) // 4\n",
    "            elif msg['role'] == 'assistant':\n",
    "                output_tokens += len(msg['content']) // 4\n",
    "        examples += 1\n",
    "    \n",
    "    # Estimate costs ($5.00 per 1M tokens)\n",
    "    input_cost = (input_tokens / 1000000) * 5.00\n",
    "    output_cost = (output_tokens / 1000000) * 5.00\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        'examples': examples,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'total_tokens': input_tokens + output_tokens,\n",
    "        'estimated_cost': total_cost\n",
    "    }\n",
    "\n",
    "# Check training data stats\n",
    "train_stats = count_tokens_in_jsonl(train_path)\n",
    "print(f\"Training data statistics:\")\n",
    "for k, v in train_stats.items():\n",
    "    print(f\" - {k}: {v}\")\n",
    "\n",
    "# Check validation data stats\n",
    "val_stats = count_tokens_in_jsonl(val_path)\n",
    "print(f\"\\nValidation data statistics:\")\n",
    "for k, v in val_stats.items():\n",
    "    print(f\" - {k}: {v}\")\n",
    "\n",
    "# Calculate total cost for 3 epochs\n",
    "n_epochs = 2\n",
    "total_cost_estimate = train_stats['estimated_cost'] * n_epochs + val_stats['estimated_cost']\n",
    "print(f\"\\nEstimated total cost for {n_epochs} epochs: ${total_cost_estimate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e3029",
   "metadata": {},
   "source": [
    "## 5. Upload Files and Start Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "508e4c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking available models for fine-tuning...\n",
      "Available GPT-4.1-mini models: ['gpt-4.1-mini', 'gpt-4.1-mini-2025-04-14']\n",
      "Uploading training file: ./data/shadertoy-1000_train.jsonl …\n",
      "Training file uploaded with ID: file-SXnJEcYEWwJBDPtoxP2Kzs\n",
      "Uploading validation file: ./data/shadertoy-1000_val.jsonl …\n",
      "Validation file uploaded with ID: file-4mdEuedwKYpwbVqXTXeZE9\n",
      "Waiting for file processing to complete…\n",
      "File file-SXnJEcYEWwJBDPtoxP2Kzs status: processed\n",
      "File file-4mdEuedwKYpwbVqXTXeZE9 status: processed\n",
      "Starting fine-tuning job with base model 'gpt-4.1-mini-2025-04-14' …\n",
      "Fine-tuning job created successfully. Job ID: ftjob-XsIS8espyrRQysFVIpASDVW5\n",
      "Job details saved to 'finetune_job_20250601_121335.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Import and instantiate the OpenAI v1 client\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1. Read your API key from the environment (or set directly)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise RuntimeError(\"Please set the OPENAI_API_KEY environment variable\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# 2. Check for available GPT-4.1-mini models\n",
    "print(\"Checking available models for fine-tuning...\")\n",
    "all_models = client.models.list().data\n",
    "available_models = [m.id for m in all_models if \"gpt-4.1-mini\" in m.id]\n",
    "print(f\"Available GPT-4.1-mini models: {available_models}\")\n",
    "\n",
    "# 3. Choose a base model (adjust if needed)\n",
    "base_model = \"gpt-4.1-mini-2025-04-14\"\n",
    "if base_model not in available_models:\n",
    "    raise RuntimeError(f\"Base model '{base_model}' not found in available models.\")\n",
    "\n",
    "# 5. Upload the training file\n",
    "print(f\"Uploading training file: {train_path} …\")\n",
    "with open(train_path, \"rb\") as train_file:\n",
    "    training_upload = client.files.create(\n",
    "        file=train_file,\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "training_file_id = training_upload.id\n",
    "print(f\"Training file uploaded with ID: {training_file_id}\")\n",
    "\n",
    "# 6. Upload the validation file\n",
    "print(f\"Uploading validation file: {val_path} …\")\n",
    "with open(val_path, \"rb\") as val_file:\n",
    "    validation_upload = client.files.create(\n",
    "        file=val_file,\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "validation_file_id = validation_upload.id\n",
    "print(f\"Validation file uploaded with ID: {validation_file_id}\")\n",
    "\n",
    "# 7. Wait for both files to be processed\n",
    "print(\"Waiting for file processing to complete…\")\n",
    "for file_id in (training_file_id, validation_file_id):\n",
    "    while True:\n",
    "        file_info = client.files.retrieve(file_id)\n",
    "        status = file_info.status\n",
    "        print(f\"File {file_id} status: {status}\")\n",
    "        if status == \"processed\":\n",
    "            break\n",
    "        time.sleep(10)\n",
    "\n",
    "# 8. Construct a unique suffix using current timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "suffix = f\"shadertoy_{timestamp}\"\n",
    "\n",
    "# 9. Create the fine-tuning job\n",
    "print(f\"Starting fine-tuning job with base model '{base_model}' …\")\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=validation_file_id,\n",
    "    model=base_model,\n",
    "    suffix=suffix,\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 2,\n",
    "        \"learning_rate_multiplier\": 0.1\n",
    "    }\n",
    ")\n",
    "job_id = job.id\n",
    "print(f\"Fine-tuning job created successfully. Job ID: {job_id}\")\n",
    "\n",
    "# 10. Save job metadata locally for later reference\n",
    "job_record = {\n",
    "    \"job_id\": job_id,\n",
    "    \"training_file\": training_file_id,\n",
    "    \"validation_file\": validation_file_id,\n",
    "    \"base_model\": base_model,\n",
    "    \"suffix\": suffix,\n",
    "    \"timestamp\": timestamp\n",
    "}\n",
    "record_filename = f\"finetune_job_{timestamp}.json\"\n",
    "with open(record_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(job_record, f, indent=2)\n",
    "print(f\"Job details saved to '{record_filename}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a0a3e",
   "metadata": {},
   "source": [
    "## 6. Monitor Fine-tuning Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7db2b558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring fine-tuning job: ftjob-XsIS8espyrRQysFVIpASDVW5\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Fine-tuning job started\n",
      "Event: Files validated, moving job to queued state\n",
      "Event: Validating training file: file-SXnJEcYEWwJBDPtoxP2Kzs and validation file: file-4mdEuedwKYpwbVqXTXeZE9\n",
      "Event: Created fine-tuning job: ftjob-XsIS8espyrRQysFVIpASDVW5\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Fine-tuning job started\n",
      "Event: Files validated, moving job to queued state\n",
      "Event: Validating training file: file-SXnJEcYEWwJBDPtoxP2Kzs and validation file: file-4mdEuedwKYpwbVqXTXeZE9\n",
      "Event: Created fine-tuning job: ftjob-XsIS8espyrRQysFVIpASDVW5\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 11/2740: training loss=1.97\n",
      "Event: Step 10/2740: training loss=1.19\n",
      "Event: Step 9/2740: training loss=1.32\n",
      "Event: Step 8/2740: training loss=0.64\n",
      "Event: Step 7/2740: training loss=0.65\n",
      "Event: Step 6/2740: training loss=0.79\n",
      "Event: Step 5/2740: training loss=2.85\n",
      "Event: Step 4/2740: training loss=1.79\n",
      "Event: Step 3/2740: training loss=1.30\n",
      "Event: Step 2/2740: training loss=1.03\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 31/2740: training loss=1.44\n",
      "Event: Step 30/2740: training loss=0.83\n",
      "Event: Step 29/2740: training loss=0.63\n",
      "Event: Step 28/2740: training loss=0.85\n",
      "Event: Step 27/2740: training loss=2.26\n",
      "Event: Step 26/2740: training loss=1.10\n",
      "Event: Step 25/2740: training loss=1.24\n",
      "Event: Step 24/2740: training loss=0.81\n",
      "Event: Step 23/2740: training loss=1.21\n",
      "Event: Step 22/2740: training loss=1.16\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 53/2740: training loss=1.08\n",
      "Event: Step 52/2740: training loss=0.84\n",
      "Event: Step 51/2740: training loss=0.37\n",
      "Event: Step 50/2740: training loss=1.81\n",
      "Event: Step 49/2740: training loss=1.11\n",
      "Event: Step 48/2740: training loss=1.12\n",
      "Event: Step 47/2740: training loss=1.24\n",
      "Event: Step 46/2740: training loss=0.62\n",
      "Event: Step 45/2740: training loss=1.08\n",
      "Event: Step 44/2740: training loss=0.52\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 77/2740: training loss=1.35\n",
      "Event: Step 76/2740: training loss=0.87\n",
      "Event: Step 75/2740: training loss=1.68\n",
      "Event: Step 74/2740: training loss=0.70\n",
      "Event: Step 73/2740: training loss=1.04\n",
      "Event: Step 72/2740: training loss=0.85\n",
      "Event: Step 71/2740: training loss=1.11\n",
      "Event: Step 70/2740: training loss=0.42\n",
      "Event: Step 69/2740: training loss=1.96\n",
      "Event: Step 68/2740: training loss=0.85\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 100/2740: training loss=0.47, validation loss=1.46\n",
      "Event: Step 99/2740: training loss=1.28\n",
      "Event: Step 98/2740: training loss=0.92\n",
      "Event: Step 97/2740: training loss=0.79\n",
      "Event: Step 96/2740: training loss=1.27\n",
      "Event: Step 95/2740: training loss=1.35\n",
      "Event: Step 94/2740: training loss=1.77\n",
      "Event: Step 93/2740: training loss=1.14\n",
      "Event: Step 92/2740: training loss=1.76\n",
      "Event: Step 91/2740: training loss=1.36\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 120/2740: training loss=1.07\n",
      "Event: Step 119/2740: training loss=0.62\n",
      "Event: Step 118/2740: training loss=0.85\n",
      "Event: Step 117/2740: training loss=0.90\n",
      "Event: Step 116/2740: training loss=1.18\n",
      "Event: Step 115/2740: training loss=1.57\n",
      "Event: Step 114/2740: training loss=1.22\n",
      "Event: Step 113/2740: training loss=3.16\n",
      "Event: Step 112/2740: training loss=0.80\n",
      "Event: Step 111/2740: training loss=0.70\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 141/2740: training loss=0.65\n",
      "Event: Step 140/2740: training loss=1.15\n",
      "Event: Step 139/2740: training loss=2.00\n",
      "Event: Step 138/2740: training loss=0.81\n",
      "Event: Step 137/2740: training loss=1.42\n",
      "Event: Step 136/2740: training loss=1.23\n",
      "Event: Step 135/2740: training loss=0.96\n",
      "Event: Step 134/2740: training loss=0.50\n",
      "Event: Step 133/2740: training loss=0.96\n",
      "Event: Step 132/2740: training loss=1.03\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 176/2740: training loss=0.74\n",
      "Event: Step 175/2740: training loss=1.35\n",
      "Event: Step 174/2740: training loss=0.96\n",
      "Event: Step 173/2740: training loss=0.62\n",
      "Event: Step 172/2740: training loss=1.03\n",
      "Event: Step 171/2740: training loss=0.63\n",
      "Event: Step 170/2740: training loss=0.88\n",
      "Event: Step 169/2740: training loss=1.64\n",
      "Event: Step 168/2740: training loss=0.62\n",
      "Event: Step 167/2740: training loss=0.50\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 201/2740: training loss=1.40\n",
      "Event: Step 200/2740: training loss=1.07, validation loss=1.24\n",
      "Event: Step 199/2740: training loss=1.56\n",
      "Event: Step 198/2740: training loss=0.68\n",
      "Event: Step 197/2740: training loss=0.77\n",
      "Event: Step 196/2740: training loss=0.86\n",
      "Event: Step 195/2740: training loss=1.01\n",
      "Event: Step 194/2740: training loss=1.73\n",
      "Event: Step 193/2740: training loss=0.97\n",
      "Event: Step 192/2740: training loss=0.58\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 229/2740: training loss=0.57\n",
      "Event: Step 228/2740: training loss=1.14\n",
      "Event: Step 227/2740: training loss=0.82\n",
      "Event: Step 226/2740: training loss=1.31\n",
      "Event: Step 225/2740: training loss=0.28\n",
      "Event: Step 224/2740: training loss=0.64\n",
      "Event: Step 223/2740: training loss=0.81\n",
      "Event: Step 222/2740: training loss=1.13\n",
      "Event: Step 221/2740: training loss=0.71\n",
      "Event: Step 220/2740: training loss=0.85\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 261/2740: training loss=0.79\n",
      "Event: Step 260/2740: training loss=0.62\n",
      "Event: Step 259/2740: training loss=0.69\n",
      "Event: Step 258/2740: training loss=0.74\n",
      "Event: Step 257/2740: training loss=1.03\n",
      "Event: Step 256/2740: training loss=0.81\n",
      "Event: Step 255/2740: training loss=0.76\n",
      "Event: Step 254/2740: training loss=0.89\n",
      "Event: Step 253/2740: training loss=0.82\n",
      "Event: Step 252/2740: training loss=0.69\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 286/2740: training loss=0.95\n",
      "Event: Step 285/2740: training loss=0.91\n",
      "Event: Step 284/2740: training loss=1.45\n",
      "Event: Step 283/2740: training loss=1.00\n",
      "Event: Step 282/2740: training loss=1.17\n",
      "Event: Step 281/2740: training loss=0.97\n",
      "Event: Step 280/2740: training loss=1.20\n",
      "Event: Step 279/2740: training loss=9.23\n",
      "Event: Step 278/2740: training loss=0.56\n",
      "Event: Step 277/2740: training loss=0.89\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 322/2740: training loss=0.59\n",
      "Event: Step 321/2740: training loss=1.01\n",
      "Event: Step 320/2740: training loss=0.67\n",
      "Event: Step 319/2740: training loss=2.63\n",
      "Event: Step 318/2740: training loss=0.91\n",
      "Event: Step 317/2740: training loss=1.12\n",
      "Event: Step 316/2740: training loss=0.14\n",
      "Event: Step 315/2740: training loss=1.12\n",
      "Event: Step 314/2740: training loss=0.99\n",
      "Event: Step 313/2740: training loss=0.93\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 349/2740: training loss=0.89\n",
      "Event: Step 348/2740: training loss=1.36\n",
      "Event: Step 347/2740: training loss=1.33\n",
      "Event: Step 346/2740: training loss=0.75\n",
      "Event: Step 345/2740: training loss=0.65\n",
      "Event: Step 344/2740: training loss=1.72\n",
      "Event: Step 343/2740: training loss=0.82\n",
      "Event: Step 342/2740: training loss=0.96\n",
      "Event: Step 341/2740: training loss=1.24\n",
      "Event: Step 340/2740: training loss=0.75\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 383/2740: training loss=0.84\n",
      "Event: Step 382/2740: training loss=1.53\n",
      "Event: Step 381/2740: training loss=0.56\n",
      "Event: Step 380/2740: training loss=2.39\n",
      "Event: Step 379/2740: training loss=0.85\n",
      "Event: Step 378/2740: training loss=1.02\n",
      "Event: Step 377/2740: training loss=0.71\n",
      "Event: Step 376/2740: training loss=0.80\n",
      "Event: Step 375/2740: training loss=0.69\n",
      "Event: Step 374/2740: training loss=1.43\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 401/2740: training loss=1.07\n",
      "Event: Step 400/2740: training loss=1.04, validation loss=0.74\n",
      "Event: Step 399/2740: training loss=1.36\n",
      "Event: Step 398/2740: training loss=0.78\n",
      "Event: Step 397/2740: training loss=1.54\n",
      "Event: Step 396/2740: training loss=1.06\n",
      "Event: Step 395/2740: training loss=1.00\n",
      "Event: Step 394/2740: training loss=0.68\n",
      "Event: Step 393/2740: training loss=0.71\n",
      "Event: Step 392/2740: training loss=0.49\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 424/2740: training loss=0.64\n",
      "Event: Step 423/2740: training loss=0.79\n",
      "Event: Step 422/2740: training loss=0.89\n",
      "Event: Step 421/2740: training loss=1.89\n",
      "Event: Step 420/2740: training loss=1.10\n",
      "Event: Step 419/2740: training loss=1.02\n",
      "Event: Step 418/2740: training loss=0.85\n",
      "Event: Step 417/2740: training loss=1.06\n",
      "Event: Step 416/2740: training loss=0.97\n",
      "Event: Step 415/2740: training loss=0.84\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 447/2740: training loss=0.94\n",
      "Event: Step 446/2740: training loss=0.87\n",
      "Event: Step 445/2740: training loss=1.00\n",
      "Event: Step 444/2740: training loss=0.88\n",
      "Event: Step 443/2740: training loss=0.56\n",
      "Event: Step 442/2740: training loss=0.87\n",
      "Event: Step 441/2740: training loss=1.31\n",
      "Event: Step 440/2740: training loss=0.56\n",
      "Event: Step 439/2740: training loss=1.03\n",
      "Event: Step 438/2740: training loss=0.60\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 483/2740: training loss=0.96\n",
      "Event: Step 482/2740: training loss=0.44\n",
      "Event: Step 481/2740: training loss=1.20\n",
      "Event: Step 480/2740: training loss=0.81\n",
      "Event: Step 479/2740: training loss=0.98\n",
      "Event: Step 478/2740: training loss=0.89\n",
      "Event: Step 477/2740: training loss=0.94\n",
      "Event: Step 476/2740: training loss=0.53\n",
      "Event: Step 475/2740: training loss=0.52\n",
      "Event: Step 474/2740: training loss=0.55\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 512/2740: training loss=1.12\n",
      "Event: Step 511/2740: training loss=1.56\n",
      "Event: Step 510/2740: training loss=0.96\n",
      "Event: Step 509/2740: training loss=0.35\n",
      "Event: Step 508/2740: training loss=1.06\n",
      "Event: Step 507/2740: training loss=0.80\n",
      "Event: Step 506/2740: training loss=0.89\n",
      "Event: Step 505/2740: training loss=1.07\n",
      "Event: Step 504/2740: training loss=0.65\n",
      "Event: Step 503/2740: training loss=1.05\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 543/2740: training loss=1.35\n",
      "Event: Step 542/2740: training loss=0.79\n",
      "Event: Step 541/2740: training loss=1.24\n",
      "Event: Step 540/2740: training loss=1.28\n",
      "Event: Step 539/2740: training loss=1.89\n",
      "Event: Step 538/2740: training loss=1.63\n",
      "Event: Step 537/2740: training loss=0.70\n",
      "Event: Step 536/2740: training loss=1.10\n",
      "Event: Step 535/2740: training loss=1.12\n",
      "Event: Step 534/2740: training loss=0.92\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 577/2740: training loss=0.94\n",
      "Event: Step 576/2740: training loss=0.72\n",
      "Event: Step 575/2740: training loss=1.21\n",
      "Event: Step 574/2740: training loss=1.58\n",
      "Event: Step 573/2740: training loss=0.91\n",
      "Event: Step 572/2740: training loss=1.05\n",
      "Event: Step 571/2740: training loss=0.74\n",
      "Event: Step 570/2740: training loss=1.25\n",
      "Event: Step 569/2740: training loss=0.93\n",
      "Event: Step 568/2740: training loss=0.96\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 613/2740: training loss=0.54\n",
      "Event: Step 612/2740: training loss=2.18\n",
      "Event: Step 611/2740: training loss=1.81\n",
      "Event: Step 610/2740: training loss=0.98\n",
      "Event: Step 609/2740: training loss=0.75\n",
      "Event: Step 608/2740: training loss=0.34\n",
      "Event: Step 607/2740: training loss=0.71\n",
      "Event: Step 606/2740: training loss=0.89\n",
      "Event: Step 605/2740: training loss=0.78\n",
      "Event: Step 604/2740: training loss=1.31\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 661/2740: training loss=0.45\n",
      "Event: Step 660/2740: training loss=1.65\n",
      "Event: Step 659/2740: training loss=0.75\n",
      "Event: Step 658/2740: training loss=0.40\n",
      "Event: Step 657/2740: training loss=1.54\n",
      "Event: Step 656/2740: training loss=0.42\n",
      "Event: Step 655/2740: training loss=1.27\n",
      "Event: Step 654/2740: training loss=0.83\n",
      "Event: Step 653/2740: training loss=1.02\n",
      "Event: Step 652/2740: training loss=1.04\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 691/2740: training loss=0.86\n",
      "Event: Step 690/2740: training loss=0.63\n",
      "Event: Step 689/2740: training loss=0.73\n",
      "Event: Step 688/2740: training loss=0.54\n",
      "Event: Step 687/2740: training loss=1.09\n",
      "Event: Step 686/2740: training loss=1.10\n",
      "Event: Step 685/2740: training loss=0.72\n",
      "Event: Step 684/2740: training loss=0.78\n",
      "Event: Step 683/2740: training loss=0.78\n",
      "Event: Step 682/2740: training loss=1.10\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 726/2740: training loss=1.59\n",
      "Event: Step 725/2740: training loss=1.33\n",
      "Event: Step 724/2740: training loss=0.79\n",
      "Event: Step 723/2740: training loss=1.32\n",
      "Event: Step 722/2740: training loss=1.24\n",
      "Event: Step 721/2740: training loss=1.14\n",
      "Event: Step 720/2740: training loss=0.97\n",
      "Event: Step 719/2740: training loss=2.07\n",
      "Event: Step 718/2740: training loss=0.66\n",
      "Event: Step 717/2740: training loss=0.81\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 762/2740: training loss=1.48\n",
      "Event: Step 761/2740: training loss=0.25\n",
      "Event: Step 760/2740: training loss=0.89\n",
      "Event: Step 759/2740: training loss=0.60\n",
      "Event: Step 758/2740: training loss=0.53\n",
      "Event: Step 757/2740: training loss=1.01\n",
      "Event: Step 756/2740: training loss=0.74\n",
      "Event: Step 755/2740: training loss=0.93\n",
      "Event: Step 754/2740: training loss=1.57\n",
      "Event: Step 753/2740: training loss=0.74\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 800/2740: training loss=0.72, validation loss=2.23\n",
      "Event: Step 799/2740: training loss=1.15\n",
      "Event: Step 798/2740: training loss=1.64\n",
      "Event: Step 797/2740: training loss=0.54\n",
      "Event: Step 796/2740: training loss=0.74\n",
      "Event: Step 795/2740: training loss=0.84\n",
      "Event: Step 794/2740: training loss=1.17\n",
      "Event: Step 793/2740: training loss=0.57\n",
      "Event: Step 792/2740: training loss=0.75\n",
      "Event: Step 791/2740: training loss=0.89\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 831/2740: training loss=0.61\n",
      "Event: Step 830/2740: training loss=1.28\n",
      "Event: Step 829/2740: training loss=1.66\n",
      "Event: Step 828/2740: training loss=0.73\n",
      "Event: Step 827/2740: training loss=0.69\n",
      "Event: Step 826/2740: training loss=1.24\n",
      "Event: Step 825/2740: training loss=0.88\n",
      "Event: Step 824/2740: training loss=1.01\n",
      "Event: Step 823/2740: training loss=0.70\n",
      "Event: Step 822/2740: training loss=1.15\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 876/2740: training loss=1.41\n",
      "Event: Step 875/2740: training loss=0.73\n",
      "Event: Step 874/2740: training loss=0.78\n",
      "Event: Step 873/2740: training loss=0.74\n",
      "Event: Step 872/2740: training loss=1.27\n",
      "Event: Step 871/2740: training loss=1.42\n",
      "Event: Step 870/2740: training loss=1.46\n",
      "Event: Step 869/2740: training loss=1.33\n",
      "Event: Step 868/2740: training loss=1.03\n",
      "Event: Step 867/2740: training loss=0.67\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 906/2740: training loss=0.58\n",
      "Event: Step 905/2740: training loss=0.69\n",
      "Event: Step 904/2740: training loss=2.09\n",
      "Event: Step 903/2740: training loss=0.71\n",
      "Event: Step 902/2740: training loss=0.22\n",
      "Event: Step 901/2740: training loss=0.56\n",
      "Event: Step 900/2740: training loss=1.31, validation loss=0.67\n",
      "Event: Step 899/2740: training loss=1.17\n",
      "Event: Step 898/2740: training loss=0.70\n",
      "Event: Step 897/2740: training loss=0.94\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 941/2740: training loss=1.20\n",
      "Event: Step 940/2740: training loss=0.52\n",
      "Event: Step 939/2740: training loss=0.53\n",
      "Event: Step 938/2740: training loss=0.93\n",
      "Event: Step 937/2740: training loss=1.06\n",
      "Event: Step 936/2740: training loss=0.73\n",
      "Event: Step 935/2740: training loss=0.48\n",
      "Event: Step 934/2740: training loss=0.96\n",
      "Event: Step 933/2740: training loss=1.40\n",
      "Event: Step 932/2740: training loss=1.70\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 981/2740: training loss=2.01\n",
      "Event: Step 980/2740: training loss=0.39\n",
      "Event: Step 979/2740: training loss=1.17\n",
      "Event: Step 978/2740: training loss=1.30\n",
      "Event: Step 977/2740: training loss=0.54\n",
      "Event: Step 976/2740: training loss=0.90\n",
      "Event: Step 975/2740: training loss=0.63\n",
      "Event: Step 974/2740: training loss=0.67\n",
      "Event: Step 973/2740: training loss=0.62\n",
      "Event: Step 972/2740: training loss=1.31\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1021/2740: training loss=0.87\n",
      "Event: Step 1020/2740: training loss=1.15\n",
      "Event: Step 1019/2740: training loss=1.08\n",
      "Event: Step 1018/2740: training loss=0.73\n",
      "Event: Step 1017/2740: training loss=0.57\n",
      "Event: Step 1016/2740: training loss=1.02\n",
      "Event: Step 1015/2740: training loss=0.92\n",
      "Event: Step 1014/2740: training loss=0.88\n",
      "Event: Step 1013/2740: training loss=0.65\n",
      "Event: Step 1012/2740: training loss=0.57\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1071/2740: training loss=0.69\n",
      "Event: Step 1070/2740: training loss=0.98\n",
      "Event: Step 1069/2740: training loss=1.37\n",
      "Event: Step 1068/2740: training loss=1.17\n",
      "Event: Step 1067/2740: training loss=0.80\n",
      "Event: Step 1066/2740: training loss=1.13\n",
      "Event: Step 1065/2740: training loss=1.17\n",
      "Event: Step 1064/2740: training loss=0.33\n",
      "Event: Step 1063/2740: training loss=1.00\n",
      "Event: Step 1062/2740: training loss=0.54\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1120/2740: training loss=1.69\n",
      "Event: Step 1119/2740: training loss=0.19\n",
      "Event: Step 1118/2740: training loss=1.41\n",
      "Event: Step 1117/2740: training loss=1.03\n",
      "Event: Step 1116/2740: training loss=1.21\n",
      "Event: Step 1115/2740: training loss=0.99\n",
      "Event: Step 1114/2740: training loss=0.87\n",
      "Event: Step 1113/2740: training loss=1.07\n",
      "Event: Step 1112/2740: training loss=0.72\n",
      "Event: Step 1111/2740: training loss=1.20\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1161/2740: training loss=1.10\n",
      "Event: Step 1160/2740: training loss=0.79\n",
      "Event: Step 1159/2740: training loss=0.75\n",
      "Event: Step 1158/2740: training loss=3.03\n",
      "Event: Step 1157/2740: training loss=0.40\n",
      "Event: Step 1156/2740: training loss=1.22\n",
      "Event: Step 1155/2740: training loss=1.10\n",
      "Event: Step 1154/2740: training loss=0.66\n",
      "Event: Step 1153/2740: training loss=0.93\n",
      "Event: Step 1152/2740: training loss=0.92\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1201/2740: training loss=0.93\n",
      "Event: Step 1200/2740: training loss=1.18, validation loss=1.30\n",
      "Event: Step 1199/2740: training loss=0.98\n",
      "Event: Step 1198/2740: training loss=1.20\n",
      "Event: Step 1197/2740: training loss=1.22\n",
      "Event: Step 1196/2740: training loss=0.76\n",
      "Event: Step 1195/2740: training loss=0.86\n",
      "Event: Step 1194/2740: training loss=0.95\n",
      "Event: Step 1193/2740: training loss=0.35\n",
      "Event: Step 1192/2740: training loss=0.88\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1261/2740: training loss=1.15\n",
      "Event: Step 1260/2740: training loss=1.41\n",
      "Event: Step 1259/2740: training loss=0.61\n",
      "Event: Step 1258/2740: training loss=0.75\n",
      "Event: Step 1257/2740: training loss=1.02\n",
      "Event: Step 1256/2740: training loss=0.71\n",
      "Event: Step 1255/2740: training loss=0.84\n",
      "Event: Step 1254/2740: training loss=0.62\n",
      "Event: Step 1253/2740: training loss=0.74\n",
      "Event: Step 1252/2740: training loss=0.86\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1302/2740: training loss=0.75\n",
      "Event: Step 1301/2740: training loss=1.06\n",
      "Event: Step 1300/2740: training loss=1.56, validation loss=1.28\n",
      "Event: Step 1299/2740: training loss=0.87\n",
      "Event: Step 1298/2740: training loss=1.10\n",
      "Event: Step 1297/2740: training loss=0.46\n",
      "Event: Step 1296/2740: training loss=1.05\n",
      "Event: Step 1295/2740: training loss=1.37\n",
      "Event: Step 1294/2740: training loss=1.00\n",
      "Event: Step 1293/2740: training loss=0.50\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1341/2740: training loss=0.76\n",
      "Event: Step 1340/2740: training loss=0.17\n",
      "Event: Step 1339/2740: training loss=2.46\n",
      "Event: Step 1338/2740: training loss=0.82\n",
      "Event: Step 1337/2740: training loss=0.89\n",
      "Event: Step 1336/2740: training loss=1.08\n",
      "Event: Step 1335/2740: training loss=1.20\n",
      "Event: Step 1334/2740: training loss=0.56\n",
      "Event: Step 1333/2740: training loss=0.86\n",
      "Event: Step 1332/2740: training loss=1.13\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1381/2740: training loss=0.77\n",
      "Event: Step 1380/2740: training loss=0.71\n",
      "Event: Step 1379/2740: training loss=1.17\n",
      "Event: Step 1378/2740: training loss=0.74\n",
      "Event: Step 1377/2740: training loss=0.72\n",
      "Event: Step 1376/2740: training loss=0.68\n",
      "Event: Step 1375/2740: training loss=0.81\n",
      "Event: Step 1374/2740: training loss=1.29\n",
      "Event: Step 1373/2740: training loss=1.04\n",
      "Event: Step 1372/2740: training loss=0.89\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1446/2740: training loss=0.99\n",
      "Event: Step 1445/2740: training loss=1.09\n",
      "Event: Step 1444/2740: training loss=1.70\n",
      "Event: Step 1443/2740: training loss=0.92\n",
      "Event: Step 1442/2740: training loss=1.54\n",
      "Event: Step 1441/2740: training loss=1.53\n",
      "Event: Step 1440/2740: training loss=0.94\n",
      "Event: Step 1439/2740: training loss=0.70\n",
      "Event: Step 1438/2740: training loss=0.61\n",
      "Event: Step 1437/2740: training loss=0.78\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1502/2740: training loss=0.97\n",
      "Event: Step 1501/2740: training loss=1.09\n",
      "Event: Step 1500/2740: training loss=0.60, validation loss=0.81\n",
      "Event: Step 1499/2740: training loss=0.73\n",
      "Event: Step 1498/2740: training loss=0.73\n",
      "Event: Step 1497/2740: training loss=0.71\n",
      "Event: Step 1496/2740: training loss=0.52\n",
      "Event: Step 1495/2740: training loss=0.74\n",
      "Event: Step 1494/2740: training loss=0.87\n",
      "Event: Step 1493/2740: training loss=1.65\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1561/2740: training loss=1.58\n",
      "Event: Step 1560/2740: training loss=0.93\n",
      "Event: Step 1559/2740: training loss=0.90\n",
      "Event: Step 1558/2740: training loss=0.74\n",
      "Event: Step 1557/2740: training loss=0.70\n",
      "Event: Step 1556/2740: training loss=0.92\n",
      "Event: Step 1555/2740: training loss=1.26\n",
      "Event: Step 1554/2740: training loss=0.54\n",
      "Event: Step 1553/2740: training loss=1.19\n",
      "Event: Step 1552/2740: training loss=0.56\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1621/2740: training loss=0.93\n",
      "Event: Step 1620/2740: training loss=0.34\n",
      "Event: Step 1619/2740: training loss=0.73\n",
      "Event: Step 1618/2740: training loss=0.94\n",
      "Event: Step 1617/2740: training loss=1.23\n",
      "Event: Step 1616/2740: training loss=0.43\n",
      "Event: Step 1615/2740: training loss=0.78\n",
      "Event: Step 1614/2740: training loss=0.80\n",
      "Event: Step 1613/2740: training loss=1.16\n",
      "Event: Step 1612/2740: training loss=1.01\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1691/2740: training loss=0.86\n",
      "Event: Step 1690/2740: training loss=0.93\n",
      "Event: Step 1689/2740: training loss=1.13\n",
      "Event: Step 1688/2740: training loss=0.74\n",
      "Event: Step 1687/2740: training loss=0.39\n",
      "Event: Step 1686/2740: training loss=0.51\n",
      "Event: Step 1685/2740: training loss=0.51\n",
      "Event: Step 1684/2740: training loss=1.43\n",
      "Event: Step 1683/2740: training loss=0.99\n",
      "Event: Step 1682/2740: training loss=0.69\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1753/2740: training loss=0.71\n",
      "Event: Step 1752/2740: training loss=0.59\n",
      "Event: Step 1751/2740: training loss=2.39\n",
      "Event: Step 1750/2740: training loss=0.90\n",
      "Event: Step 1749/2740: training loss=1.77\n",
      "Event: Step 1748/2740: training loss=0.84\n",
      "Event: Step 1747/2740: training loss=1.19\n",
      "Event: Step 1746/2740: training loss=2.44\n",
      "Event: Step 1745/2740: training loss=0.75\n",
      "Event: Step 1744/2740: training loss=2.09\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1811/2740: training loss=1.69\n",
      "Event: Step 1810/2740: training loss=0.79\n",
      "Event: Step 1809/2740: training loss=0.72\n",
      "Event: Step 1808/2740: training loss=0.65\n",
      "Event: Step 1807/2740: training loss=0.95\n",
      "Event: Step 1806/2740: training loss=0.79\n",
      "Event: Step 1805/2740: training loss=0.69\n",
      "Event: Step 1804/2740: training loss=1.31\n",
      "Event: Step 1803/2740: training loss=0.91\n",
      "Event: Step 1802/2740: training loss=0.70\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1879/2740: training loss=0.93\n",
      "Event: Step 1878/2740: training loss=1.13\n",
      "Event: Step 1877/2740: training loss=0.88\n",
      "Event: Step 1876/2740: training loss=1.02\n",
      "Event: Step 1875/2740: training loss=0.97\n",
      "Event: Step 1874/2740: training loss=1.35\n",
      "Event: Step 1873/2740: training loss=1.06\n",
      "Event: Step 1872/2740: training loss=1.12\n",
      "Event: Step 1871/2740: training loss=1.25\n",
      "Event: Step 1870/2740: training loss=0.64\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 1941/2740: training loss=0.55\n",
      "Event: Step 1940/2740: training loss=1.14\n",
      "Event: Step 1939/2740: training loss=0.53\n",
      "Event: Step 1938/2740: training loss=0.69\n",
      "Event: Step 1937/2740: training loss=0.18\n",
      "Event: Step 1936/2740: training loss=0.79\n",
      "Event: Step 1935/2740: training loss=1.42\n",
      "Event: Step 1934/2740: training loss=1.54\n",
      "Event: Step 1933/2740: training loss=1.09\n",
      "Event: Step 1932/2740: training loss=0.56\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 2001/2740: training loss=0.96\n",
      "Event: Step 2000/2740: training loss=1.41, validation loss=1.46\n",
      "Event: Step 1999/2740: training loss=0.69\n",
      "Event: Step 1998/2740: training loss=0.76\n",
      "Event: Step 1997/2740: training loss=0.54\n",
      "Event: Step 1996/2740: training loss=1.30\n",
      "Event: Step 1995/2740: training loss=0.88\n",
      "Event: Step 1994/2740: training loss=0.46\n",
      "Event: Step 1993/2740: training loss=1.21\n",
      "Event: Step 1992/2740: training loss=0.30\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 2066/2740: training loss=0.42\n",
      "Event: Step 2065/2740: training loss=0.62\n",
      "Event: Step 2064/2740: training loss=2.10\n",
      "Event: Step 2063/2740: training loss=0.96\n",
      "Event: Step 2062/2740: training loss=0.83\n",
      "Event: Step 2061/2740: training loss=1.56\n",
      "Event: Step 2060/2740: training loss=0.83\n",
      "Event: Step 2059/2740: training loss=0.54\n",
      "Event: Step 2058/2740: training loss=1.25\n",
      "Event: Step 2057/2740: training loss=1.37\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 2131/2740: training loss=0.62\n",
      "Event: Step 2130/2740: training loss=0.43\n",
      "Event: Step 2129/2740: training loss=1.01\n",
      "Event: Step 2128/2740: training loss=0.98\n",
      "Event: Step 2127/2740: training loss=1.70\n",
      "Event: Step 2126/2740: training loss=1.09\n",
      "Event: Step 2125/2740: training loss=0.71\n",
      "Event: Step 2124/2740: training loss=0.55\n",
      "Event: Step 2123/2740: training loss=0.68\n",
      "Event: Step 2122/2740: training loss=1.08\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 2191/2740: training loss=0.85\n",
      "Event: Step 2190/2740: training loss=1.68\n",
      "Event: Step 2189/2740: training loss=0.93\n",
      "Event: Step 2188/2740: training loss=0.79\n",
      "Event: Step 2187/2740: training loss=0.24\n",
      "Event: Step 2186/2740: training loss=0.73\n",
      "Event: Step 2185/2740: training loss=1.00\n",
      "Event: Step 2184/2740: training loss=1.12\n",
      "Event: Step 2183/2740: training loss=1.22\n",
      "Event: Step 2182/2740: training loss=9.27\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Step 2271/2740: training loss=0.55\n",
      "Event: Step 2270/2740: training loss=0.86\n",
      "Event: Step 2269/2740: training loss=1.41\n",
      "Event: Step 2268/2740: training loss=1.15\n",
      "Event: Step 2267/2740: training loss=0.62\n",
      "Event: Step 2266/2740: training loss=0.90\n",
      "Event: Step 2265/2740: training loss=1.35\n",
      "Event: Step 2264/2740: training loss=1.08\n",
      "Event: Step 2263/2740: training loss=0.42\n",
      "Event: Step 2262/2740: training loss=0.77\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Evaluating model against our usage policies before enabling\n",
      "Event: New fine-tuned model created\n",
      "Event: Checkpoint created at step 1370\n",
      "Event: Step 2740/2740: training loss=1.32, validation loss=0.87, full validation loss=0.88\n",
      "Event: Step 2739/2740: training loss=0.76\n",
      "Event: Step 2738/2740: training loss=1.78\n",
      "Event: Step 2737/2740: training loss=0.67\n",
      "Event: Step 2736/2740: training loss=0.89\n",
      "Event: Step 2735/2740: training loss=1.42\n",
      "Event: Step 2734/2740: training loss=1.29\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Evaluating model against our usage policies before enabling\n",
      "Event: New fine-tuned model created\n",
      "Event: Checkpoint created at step 1370\n",
      "Event: Step 2740/2740: training loss=1.32, validation loss=0.87, full validation loss=0.88\n",
      "Event: Step 2739/2740: training loss=0.76\n",
      "Event: Step 2738/2740: training loss=1.78\n",
      "Event: Step 2737/2740: training loss=0.67\n",
      "Event: Step 2736/2740: training loss=0.89\n",
      "Event: Step 2735/2740: training loss=1.42\n",
      "Event: Step 2734/2740: training loss=1.29\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Evaluating model against our usage policies before enabling\n",
      "Event: New fine-tuned model created\n",
      "Event: Checkpoint created at step 1370\n",
      "Event: Step 2740/2740: training loss=1.32, validation loss=0.87, full validation loss=0.88\n",
      "Event: Step 2739/2740: training loss=0.76\n",
      "Event: Step 2738/2740: training loss=1.78\n",
      "Event: Step 2737/2740: training loss=0.67\n",
      "Event: Step 2736/2740: training loss=0.89\n",
      "Event: Step 2735/2740: training loss=1.42\n",
      "Event: Step 2734/2740: training loss=1.29\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Evaluating model against our usage policies before enabling\n",
      "Event: New fine-tuned model created\n",
      "Event: Checkpoint created at step 1370\n",
      "Event: Step 2740/2740: training loss=1.32, validation loss=0.87, full validation loss=0.88\n",
      "Event: Step 2739/2740: training loss=0.76\n",
      "Event: Step 2738/2740: training loss=1.78\n",
      "Event: Step 2737/2740: training loss=0.67\n",
      "Event: Step 2736/2740: training loss=0.89\n",
      "Event: Step 2735/2740: training loss=1.42\n",
      "Event: Step 2734/2740: training loss=1.29\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Evaluating model against our usage policies before enabling\n",
      "Event: New fine-tuned model created\n",
      "Event: Checkpoint created at step 1370\n",
      "Event: Step 2740/2740: training loss=1.32, validation loss=0.87, full validation loss=0.88\n",
      "Event: Step 2739/2740: training loss=0.76\n",
      "Event: Step 2738/2740: training loss=1.78\n",
      "Event: Step 2737/2740: training loss=0.67\n",
      "Event: Step 2736/2740: training loss=0.89\n",
      "Event: Step 2735/2740: training loss=1.42\n",
      "Event: Step 2734/2740: training loss=1.29\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Evaluating model against our usage policies before enabling\n",
      "Event: New fine-tuned model created\n",
      "Event: Checkpoint created at step 1370\n",
      "Event: Step 2740/2740: training loss=1.32, validation loss=0.87, full validation loss=0.88\n",
      "Event: Step 2739/2740: training loss=0.76\n",
      "Event: Step 2738/2740: training loss=1.78\n",
      "Event: Step 2737/2740: training loss=0.67\n",
      "Event: Step 2736/2740: training loss=0.89\n",
      "Event: Step 2735/2740: training loss=1.42\n",
      "Event: Step 2734/2740: training loss=1.29\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Evaluating model against our usage policies before enabling\n",
      "Event: New fine-tuned model created\n",
      "Event: Checkpoint created at step 1370\n",
      "Event: Step 2740/2740: training loss=1.32, validation loss=0.87, full validation loss=0.88\n",
      "Event: Step 2739/2740: training loss=0.76\n",
      "Event: Step 2738/2740: training loss=1.78\n",
      "Event: Step 2737/2740: training loss=0.67\n",
      "Event: Step 2736/2740: training loss=0.89\n",
      "Event: Step 2735/2740: training loss=1.42\n",
      "Event: Step 2734/2740: training loss=1.29\n",
      "Waiting 60 seconds for next update...\n",
      "Status: running\n",
      "Trained tokens: None\n",
      "Event: Evaluating model against our usage policies before enabling\n",
      "Event: New fine-tuned model created\n",
      "Event: Checkpoint created at step 1370\n",
      "Event: Step 2740/2740: training loss=1.32, validation loss=0.87, full validation loss=0.88\n",
      "Event: Step 2739/2740: training loss=0.76\n",
      "Event: Step 2738/2740: training loss=1.78\n",
      "Event: Step 2737/2740: training loss=0.67\n",
      "Event: Step 2736/2740: training loss=0.89\n",
      "Event: Step 2735/2740: training loss=1.42\n",
      "Event: Step 2734/2740: training loss=1.29\n",
      "Waiting 60 seconds for next update...\n",
      "Status: succeeded\n",
      "Trained tokens: 7947916\n",
      "Job succeeded!\n",
      "Fine-tuned model: ft:gpt-4.1-mini-2025-04-14:soham-konar-projects:shadertoy-20250601-121335:BdjVWKmX\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "def monitor_job_v1(job_id, poll_interval=60):\n",
    "    \"\"\"Monitor fine-tuning job progress using OpenAI Python SDK v1.x+\"\"\"\n",
    "    print(f\"Monitoring fine-tuning job: {job_id}\")\n",
    "    \n",
    "    while True:\n",
    "        job_info = openai.fine_tuning.jobs.retrieve(job_id)\n",
    "        status = job_info.status\n",
    "        print(f\"Status: {status}\")\n",
    "        \n",
    "        if hasattr(job_info, \"trained_tokens\"):\n",
    "            print(f\"Trained tokens: {job_info.trained_tokens}\")\n",
    "        \n",
    "        if status in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
    "            print(f\"Job {status}!\")\n",
    "            if status == \"succeeded\":\n",
    "                print(f\"Fine-tuned model: {job_info.fine_tuned_model}\")\n",
    "                # Save the model ID\n",
    "                with open(f\"finetune_complete_{job_id}.json\", 'w') as f:\n",
    "                    json.dump({\n",
    "                        \"job_id\": job_id,\n",
    "                        \"fine_tuned_model\": job_info.fine_tuned_model,\n",
    "                        \"status\": status\n",
    "                    }, f, indent=2)\n",
    "            return job_info\n",
    "        \n",
    "        # Get events to see training metrics\n",
    "        events = openai.fine_tuning.jobs.list_events(job_id, limit=10)\n",
    "        for event in events.data:\n",
    "            print(f\"Event: {event.message}\")\n",
    "        \n",
    "        print(f\"Waiting {poll_interval} seconds for next update...\")\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "job_info = monitor_job_v1(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c2bf0",
   "metadata": {},
   "source": [
    "## 7. Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the fine-tuned model\n",
    "def test_fine_tuned_model(model_id, test_prompts):\n",
    "    \"\"\"Test the fine-tuned model with example prompts\"\"\"\n",
    "    print(f\"Testing fine-tuned model: {model_id}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"\\n--- Test Example {i+1} ---\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        \n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a GLSL expert. Generate\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                max_tokens=1500\n",
    "            )\n",
    "            \n",
    "            generated_code = response.choices[0].message.content\n",
    "            print(f\"Generated code (first 200 chars):\\n{generated_code[:200]}...\")\n",
    "            \n",
    "            results.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": generated_code\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    # Save results\n",
    "    with open(f\"test_results_{model_id.split(':')[-1]}.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example test prompts\n",
    "test_prompts = [\n",
    "    \"Title: Neon Tunnel\\nDescription: A rotating neon tunnel effect with RGB glow.\",\n",
    "    \"Title: Ocean Waves\\nDescription: Realistic ocean wave simulation with foam and reflections.\",\n",
    "    \"Title: Fractal Explorer\\nDescription: Interactive mandelbrot fractal with zoom and color controls.\"\n",
    "]\n",
    "\n",
    "# To test the model:\n",
    "# Uncomment and run when your model is ready\n",
    "# Replace with your actual model ID\n",
    "model_id = \"gpt-4.1-mini\"\n",
    "test_results = test_fine_tuned_model(model_id, test_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f6414",
   "metadata": {},
   "source": [
    "## 8. Analyze Results and Fine-tuning Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d862dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze fine-tuning metrics\n",
    "def analyze_training_metrics(job_id):\n",
    "    \"\"\"Retrieve and visualize training metrics for a fine-tuning job\"\"\"\n",
    "    events = openai.FineTuningJob.list_events(id=job_id)\n",
    "    metrics = []\n",
    "    \n",
    "    for event in events.data:\n",
    "        if \"train/train_loss\" in event.message or \"valid/loss\" in event.message:\n",
    "            # Parse the metrics (this is a bit hacky, might need adjustments)\n",
    "            parts = event.message.split()\n",
    "            for part in parts:\n",
    "                if \"=\" in part:\n",
    "                    key, value = part.split(\"=\")\n",
    "                    try:\n",
    "                        metrics.append({\n",
    "                            \"metric\": key.strip(),\n",
    "                            \"value\": float(value.strip()),\n",
    "                            \"step\": len(metrics) + 1\n",
    "                        })\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "    \n",
    "    # Group by metric type\n",
    "    metric_types = {}\n",
    "    for m in metrics:\n",
    "        if m[\"metric\"] not in metric_types:\n",
    "            metric_types[m[\"metric\"]] = []\n",
    "        metric_types[m[\"metric\"]].append((m[\"step\"], m[\"value\"]))\n",
    "    \n",
    "    # Plot metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for metric, values in metric_types.items():\n",
    "        steps, vals = zip(*values)\n",
    "        plt.plot(steps, vals, marker='o', label=metric)\n",
    "    \n",
    "    plt.title('Fine-tuning Metrics')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# To analyze metrics:\n",
    "# Uncomment and run when your job is complete\n",
    "# metrics = analyze_training_metrics(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c438d5eb",
   "metadata": {},
   "source": [
    "## 9. Integration with Shader Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function to integrate the fine-tuned model with your shader application\n",
    "def generate_shader(model_id, title, description):\n",
    "    \"\"\"Generate a shader using the fine-tuned model\"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a GLSL expert.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Title: {title}\\nDescription: {description}\"}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=2000\n",
    "        )\n",
    "        \n",
    "        generated_code = response.choices[0].message.content\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"code\": generated_code\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Example usage in your application:\n",
    "\"\"\"\n",
    "# Replace with your actual model ID\n",
    "model_id = \"ft:gpt-4.1-mini:shadertoy:12345\"\n",
    "\n",
    "# This would be connected to your UI\n",
    "title = \"Sunset Ocean\"\n",
    "description = \"A peaceful ocean scene at sunset with reflective water and soft waves\"\n",
    "\n",
    "result = generate_shader(model_id, title, description)\n",
    "if result[\"success\"]:\n",
    "    # Send to your shader renderer\n",
    "    shader_code = result[\"code\"]\n",
    "    # ... render the shader ...\n",
    "else:\n",
    "    print(f\"Error generating shader: {result['error']}\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iterative-shader-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
